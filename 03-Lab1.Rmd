# Lab exercise 1: Understanding the recent past {#lab1}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 03-Lab1.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
source("R/init_python.R")
source("R/deco_hook.R")
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
mpl.use("agg") # Needed for the book only
mpl.rc('font', size = 12)
from matplotlib import pyplot as plt
plt.switch_backend('agg') # Might be overkill
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 10)
import scipy.stats as stats
import ec3

ec3.find_station() # run here to suppress warning
```

One of the first steps of a climate change impact assessment (CCIA) is to establish a climatological baseline for the climate variable of interest (either an exposure unit or the climate variable most directly linked to the exposure unit). 

An __exposure unit__ is an element or variable that is directly or indirectly affected by the changes in climate. In this exercise you will acquire Canadian climate data for Toronto and perform an analysis of an exposure unit that will be assigned to you. Refer to Appendix \@ref(appendix) for a list of some example exposure units. 

What do we mean by baseline? A baseline period is needed to define the observed climate. This observed climate information is combined with GCM output of climate change to determine the climate change impact for a given future climate scenario (e.g. Representative Concentration Pathway (RCP)). When using climate model output, the baseline also serves as the reference period from which the modelled future change in climate is calculated. Future climate change is defined by comparing the years in the baseline period with the similar number of years in the future for a given future climate scenario.

Environment and Climate Change Canada (ECCC) makes data available over an internet portal. You can [visit this page](http://climate.weather.gc.ca/), and download data manually, but that is a _very_ tedious process. In this lab, I will present you with two options for downloading Canadian climate data: the command-line, and via an R package. 

## Getting the data

ECCC makes data available over an [FTP site](ftp://client_climate@ftp.tor.ec.gc.ca/Pub/Get_More_Data_Plus_de_donnees/), and gives instructions for downloading data manually via `bash` and `wget`. You can find instructions for using the command line to download data [here](ftp://client_climate@ftp.tor.ec.gc.ca/Pub/Get_More_Data_Plus_de_donnees/Readme.txt).

#### An all-in-Python solution

For convenience, I have written a small Python module, called **ec3** to search for and download data from the ECCC archive. You can review the source code for this module on [GitLab](https://gitlab.com/ConorIA/ec3.py). **ec3** is also available as a stand-alone command-line program, so if you have a colleague who is afraid to learn how to code, they can still use **ec3** to speed up the process of downloading data from ECCC. 

If you followed the tip box in Section \@ref(enviro), then you already have **ec3** installed. If you didn't, you can install it via conda. Note that **ec3** is not available in the official Anaconda channels, and has some dependencies from the "conda-forge" channel. I'll prepend conda-forge to the start of my channels list so that packages get installed from there first, and append my personal channel to the end of the list so that it is only used for packages that are not available in any other channel.

```{bash install_ec3, eval=FALSE, deco=list(label="Shell", bc="#000000", tc="#ffffff", icon=list(style="fas", name="terminal"))}
conda config --prepend channels conda-forge
conda config --append channels ConorIA
conda install ec3
```

Once you have the **ec3** module installed, import it.

```{python import_ec3, eval=FALSE, deco=list()}
import ec3
```

The **ec3** module contains two functions: `ec3.find_station()`, and
`ec3.get_data()`. The functions have complete docstrings, so you can check the function documentation for syntax, e.g. 

```{python ec3_help, deco=list()}
print(ec3.get_data.__doc__)
```

Let's download the daily data for the "Toronto" station (No. 5051) from 1981 to 2010. We can request the dates that we want using a Python range. Remember that Python ranges are "left inclusive, right exclusive". That means that `range(1981, 2011)` will give us data from 1981 up to (but not including) 2011. _Note that I am turning off the progress bar here because it interferes with the book rendering. You can omit the last argument._

```{python get_tor, deco=list()}
tor = ec3.get_data(stations=5051, years=range(1981, 2011))
```


#### Cleaning the data

Now that we have the data that we need, we can start working with it. First, let's import the libraries we are going to need to manipulate our data. 

```{python import_libs, eval=FALSE, deco=list()}
import numpy as np
import pandas as pd
```

Let's take a look at what our columns are named. 

```{python print_cols, deco=list()}
print(tor.columns)
```

Hmm, some of those are are a little cumbersome. Let's rename the columns we are going to use and then select only those columns. 

```{python rename_cols, deco=list()}
tor = tor.rename(columns={'Date/Time': 'Date', 'Max Temp (°C)': 'MaxTemp', 'Min Temp (°C)': 'MinTemp', 'Mean Temp (°C)': 'MeanTemp', 'Total Precip (mm)': 'TotalPrecip'})
tor = tor[['Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

Great. Let's take a look at what we've got. 

```{python tor_head, deco=list()}
print(tor.head())
```
```{python tor_tail, deco=list()}
print(tor.tail())
```

Uhh ohh! It looks like we are missing all of our temperature data at the end of our baseline period. We can test this theory by using an enumerator. The following will tell us the index at which all three of our temperature variables of interest are missing for the first time. 
```{python first_missing, deco=list()}
print(next(i for i, x in enumerate(tor.loc[:, 'MaxTemp':'MeanTemp'].isnull().all(axis=1)) if x) - 1)
```

Let's take a look at the context around that index. 

```{python range_missing, deco=list()}
print(tor[8211:8221])
```

It looks like we're missing data as of July 2003! Something odd has happened. Let's look for a nearby station (within 2 km) that we can use!

```{python find_station, deco=list()}
print(ec3.find_station(target=5051, dist=range(3)))
```

By the looks of it, "Toronto" and "Toronto City" are co-located! Indeed, in 2003 the "Toronto" station was renamed "Toronto City" and re-coded. Since that date, the daily data is available under station code 31688. 

Let's grab that dataset as of 2000 and compare with station 5051.

```{python get_tor2, deco=list()}
tor2 = ec3.get_data(stations=31688, years=range(2000, 2011), progress=False)
```

The ECCC data has a standard format, so our column names for `tor2` are going to be the same nasty column names that we had in `tor`. Let's rename them and select the columns we are interested in.

```{python rename_cols2, deco=list()}
tor2 = tor2.rename(columns={'Date/Time': 'Date', 'Max Temp (°C)': 'MaxTemp', 'Min Temp (°C)': 'MinTemp', 'Mean Temp (°C)': 'MeanTemp', 'Total Precip (mm)': 'TotalPrecip'})
tor2 = tor2[['Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

We can check the head and the tail of this data too. 

```{python tor2_head, deco=list()}
print(tor2.head())
```
```{python tor2_tail, deco=list()}
print(tor2.tail())
```

As expected, we're missing data at the beginning of the file (before 2003). We can use a slightly modified version of our iterator from earlier. This time, we'll check for the first row for which any of the data is present. 

```{python first_present, deco=list()}
print(next(i for i, x in enumerate(tor2.loc[:, 'MaxTemp':'MeanTemp'].notnull().any(axis=1)) if x) - 1)
```

We'll take a look at that in context again. 

```{python range_present, deco=list()}
print(tor2[880:890])
```

It looks like our data starts as of June 04, 2002. 

OK, so we know that we have data at the old station until May 30, 2002. Likewise, we have some data at the new station in May 2002. Let's take a look at the overlap and see if we can merge these files without issue. 

```{python compare1, deco=list()}
print(tor[(tor.Date >= "2003-06-25") & (tor.Date <= "2003-07-05")])
```
```{python compare2, deco=list()}
print(tor2[(tor2.Date >= "2003-06-25") & (tor2.Date <= "2003-07-05")])
```

The data on the overlapping days is virtually identical between the two station codes, so we can be confident that merging these two data sets won't lead to a sudden temperature bump. Let's append the relevant section of `tor2` to `tor`.

```{python merge_tors, deco=list()}
tor = tor[tor.Date < "2003-07-01"].append(tor2[tor2.Date >= "2003-07-01"])
```

Now we can make sure that we have a full data set from January 1981 to December 2010. 

```{python merged_head, deco=list()}
print(tor.head())
```
```{python merged_tail, deco=list()}
print(tor.tail())
```

Save your new data set to _csv_ by:

```{python save_csv, deco=list()}
tor.to_csv("tor.csv")
```


## Analyzing the data

Now let's take a look at our data. We will require some additional libraries.

```{python import_libs2, eval=FALSE, deco=list()}
import matplotlib as mpl
mpl.rc('font', size = 12)
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import scipy.stats as stats
```

As we mentioned in the introduction, an exposure unit is often the "unit" of our analysis when we want to perform an impact assessment. In this section, we'll calculate two exposure units, starting with a rather simple Boolean (True/False) unit. 

As I write this section, the temperature outside is a sweltering 32 °C (36 with the humidex), so what better exposure unit to consider than "tropical nights", nights with a minimum temperature above 20 °C. We will limit our analysis to the summer months.

This code requires that your "Date" column is recognized as such by Python. This isn't always automatic, so you will likely need to ask **pandas** to convert the column to a `datetime` object. 

```{python fix_date, deco=list()}
tor.Date = pd.to_datetime(tor.Date)
```

Now let's start by adding a new column to our table called "TropNight", which will be a True/False value.

```{python calc_trop, deco=list()}
tor['TropNight'] = ((tor.MinTemp > 20) & (tor.Date.dt.month.isin([6, 7, 8])))
print(tor.head())
```

Since our value is Boolean, we can use that column as an index. Let's double-check to make sure that we got the results that we were expecting.

```{python trop_head, deco=list()}
print(tor[tor.TropNight].head())
```

Now let's get an aggregate data set by grouping by the year. 

```{python agg_trop, deco=list()}
tor_trop = tor[['Date', 'TropNight']].groupby(tor.Date.dt.year).sum()
print(tor_trop.head())
```

We can plot this easily. Note, this book is generated on a machine with no display, if you want to see the plot, run `plot.show()` or set the option `%matplotlib inline` if you are using Jupyter Notebook.

```{python create_l1f1, deco=list()}
tor_trop.plot()
```
```{python save_l1f1, include=FALSE}
plt.savefig('l1f1.png')
plt.clf()
```

```{r l1f1, echo=FALSE, fig.cap="Total summertime tropical nights at Toronto (1981\u20122010)."}
knitr::include_graphics("l1f1.png", dpi = NA)
```

The grouping operation has changed our table index to the years from 1981 to 2010. We want to keep this information in the table, so we can reset our index like this:

```{python fix_index, deco=list()}
tor_trop.reset_index(level = 0, inplace = True)
print(tor_trop.head())
```

Now let's make another plot, this time with a trendline. 

```{python create_l1f2, deco=list()}
plt.plot(tor_trop.Date, tor_trop.TropNight)
plt.plot(tor_trop.Date, np.polyval(np.polyfit(tor_trop.Date, tor_trop.TropNight, 1), tor_trop.Date))
plt.title("Total summertime Tropical Nights at Toronto (1981‒2010)")
plt.xlabel("Year")
plt.ylabel("No. of Tropical Nights")
```
```{python save_l1f2, include = FALSE}
plt.savefig('l1f2.png')
plt.clf()
```

```{r l1f2, echo=FALSE, fig.cap="Total summertime tropical nights at Toronto (1981\u20122010), with linear trend line."}
knitr::include_graphics("l1f2.png", dpi = NA)
```

Let's try for a slightly more complicated exposure unit: Winter heating degree days (HDDs). The HDDs represent the difference between the mean temperature and the base temperature of 18 °C. Since HDDs can't be negative, they are only calculated when the mean temperature is below 18 °C. They are used as an indirect measure of energy demand. 

We can apply an anonymous (lambda) function to our `MinTemp` column to capture the following pseudocode expression:

<center>`return 0 if the mean temperature was above 18, otherwise, return the difference between 18 and the mean temperature`</center>

```{python calc_hdd, deco=list()}
tor['HDD'] = tor.MeanTemp.apply(lambda x: 0 if x >= 18 else 18 - x)
```

We have one more challenge to overcome. A common mistake when performing seasonal analysis on the winter is to simply group December, January, and February by year. This is an easy logical jump to make. Remember, when we refer to winter, we mean the _continuous_ winter season that stretches from the December of the previous year to February of the current year. The easiest way to control for this is to add 1 to the year for any December. Winter 1981/82, for example will then appear with `'Year'` 1982 in the data frame. 

```{python fix_win, deco=list()}
tor['Year'] = tor.Date.apply(lambda x: x.year + 1 if x.month == 12 else x.year)
```

Now we can aggregate our heating degree days. Pay close attention to the code below. First, I use two conditional filters in the square brackets. The first drops the two winters that I know are incomplete: winter 1980/81 (which is missing December 1980), and winter 2010/2011, which is really just December 2010 here. A better solution for the first issue would be to download data including December 1980. The second condition filter to just our winter months, 12, 1, and 2. Next, I select only the two columns that are of interest, group them by `'Year'` and aggregate by `sum()`. Finally, I reset the index. 

```{python agg_hdd, deco=list()}
tor_hdd = tor[tor.Year.isin(range(1982, 2011)) & tor.Date.dt.month.isin([12, 1, 2])][['Year', 'HDD']].groupby('Year').sum().reset_index()
print(tor_hdd.head())
```

```{python create_l1f3, deco=list()}
plt.plot(tor_hdd.Year, tor_hdd.HDD)
plt.plot(tor_hdd.Year, np.polyval(np.polyfit(tor_hdd.Year, tor_hdd.HDD, 1), tor_hdd.Year))
plt.title("Winter HDD at Toronto (1981/82‒2009/10)")
plt.xlabel("Year")
plt.ylabel("Heating Degree Days  (HDD)")
```
```{python save_l1f3, include = FALSE}
plt.savefig('l1f3.png')
plt.clf()
```

```{r l1f3, echo=FALSE, fig.cap="Total wintertime heating degree days (HDD) at Toronto (1981\u20122010), with linear trend line."}
knitr::include_graphics("l1f3.png", dpi = NA)
```

Let's see if we can detect any trends in our baseline values. For convenience, I am going to create a function that will spit out the relevant values. You can run these commands directly, but this might save you time when you get to the exercises (Hint!).

```{python def_test_trends, deco=list()}
def test_trends(years, values):
    slope, intercept, r_value, p_value, std_err = stats.linregress(years, values)
    print("The regression coefficients are", np.round(slope, 3), "for the slope and",
          np.round(intercept, 1), "for the intercept\n")

    t_crit = stats.t.ppf(0.975, len(years) - 1)
    confidence_interval = t_crit * std_err
    print("The true value of the slope is then", np.round(slope, 3), "+/-",
          np.round(confidence_interval, 3),"\n")

    pearsons_corrcoef, p_corr = stats.pearsonr(years,  values)
    levels = [0.001, 0.01, 0.05, 0.1]
    lvl = [i for i in levels if p_corr < i]
    print("The correlation is", np.round(pearsons_corrcoef, 3), "with a p-value of",
          np.round(p_corr, 5), "(not significant)\n" if lvl == [] else
          "(significant at the " + str(min(lvl)) + " level)\n")

    print("The variance in", values.name, "explained by the linear trend is",
          "quantified by the R²: R² =",
          np.round(100 * pearsons_corrcoef**2, 3), '%.\n')
```


```{python trends_trop, deco=list()}
test_trends(tor_trop.Date, tor_trop.TropNight)
```

```{python trends_hdd, deco=list()}
test_trends(tor_hdd.Year, tor_hdd.HDD)
```

## Exercises (what to submit)

```{block, type='rmdassignment'}
- You have been assigned an exposure unit and period. Write a brief description of the exposure unit, whether there were any missing or suspicious data, and how these data were treated. [2 marks] 
- Include the time series plots with fitted trend lines for your exposure unit and for the relevant temperature variable, e.g. `MeanTemp` for freezing degree days. [2 marks] 
- Include a table with the summary statistics for your exposure unit and temperature variable. [2 marks] 
- Now that we know that Station 5051 was recoded sometime between 2002 and 2003, future analyses might be quicker if we simply merge the 5051 data up to 2002 and the 31688 data from 2003 onward. Does this change in the join date between the two stations affect your results? [2 marks] 
- Be sure to clearly label your plots and table and include concise figure and table captions. [2 marks]
```
