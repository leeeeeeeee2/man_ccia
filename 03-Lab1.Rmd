# Lab exercise 1: Understanding the recent past {#lab1}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 03-Lab1.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
source("R/init_python.R")
source("R/deco_hook.R")
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
mpl.rc('font', size = 12)
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 10)
import scipy.stats as stats
import ec3

ec3.find_station() # run here to suppress warning
```

One of the first steps of a climate change impact assessment (CCIA) is to establish a climatological baseline for the climate variable of interest (either an exposure unit or the climate variable most directly linked to the exposure unit). 

An __exposure unit__ is an element or variable that is directly or indirectly affected by the changes in climate. In this exercise you will acquire Canadian climate data for Toronto and perform an analysis of an exposure unit that will be assigned to you. Refer to Appendix \@ref(appendix) for a list of some example exposure units. 

What do we mean by baseline? A baseline period is needed to define the observed climate. This observed climate information is combined with GCM output of climate change to determine the climate change impact for a given future climate scenario (e.g. Representative Concentration Pathway (RCP)). When using climate model output, the baseline also serves as the reference period from which the modelled future change in climate is calculated. Future climate change is defined by comparing the years in the baseline period with the similar number of years in the future for a given future climate scenario.

Environment and Climate Change Canada (ECCC) makes data available over an internet portal. You can [visit this page](http://climate.weather.gc.ca/), and download data manually, but that is a _very_ tedious process. In this lab, we will use a custom Python module to make it easier to download these data in bulk. 

## Getting the data

ECCC makes data available via a web server and gives instructions for downloading data manually via Bash and Wget. You can find instructions for using the command line to download data on [Google Drive](https://drive.google.com/drive/folders/1WJCDEU34c60IfOnG4rv5EPZ4IhhW9vZH/).

For convenience, I have written a small Python module, called **ec3** to search for and download data from the ECCC archive. You can review the source code for this module on [GitLab](https://gitlab.com/ConorIA/ec3.py). **ec3** is also available as a stand-alone command-line program, so if you have a colleague who is afraid to learn how to code, they can still use **ec3** to speed up the process of downloading data from ECCC. 

If you followed the instructions in Section \@ref(enviro), then you already have **ec3** installed. If you didn't, you can install it via Conda. Note that **ec3** is not available in the official Anaconda channels, and has some dependencies from the "conda-forge" channel. First, _prepend_ conda-forge to the start of your channels list so that packages get installed from there first, and _append_ Conor's personal channel to the end of the list so that it is only used for packages that are not available in any other channel.

```{bash install_ec3, eval=FALSE, deco=list(label="Shell", bc="#000000", tc="#ffffff", icon=list(style="fas", name="terminal"))}
conda config --prepend channels conda-forge
conda config --append channels ConorIA
conda install ec3
```

Once you have the **ec3** module installed, import it.

```{python import_ec3, eval=FALSE, deco=list()}
import ec3
```

The **ec3** module contains two functions: `ec3.find_station()`, and
`ec3.get_data()`. The functions have complete docstrings, so you can check the function documentation for syntax, e.g. 

```{python ec3_help, deco=list()}
print(ec3.get_data.__doc__)
```

Let's download the daily data for the "Toronto" station (No. 5051) from 1981 to 2010. We can request the dates that we want using a Python range. Remember that Python ranges are "left inclusive, right exclusive". That means that `range(1981, 2011)` will give us data from 1981 up to (but not including) 2011. _Note that I am turning off the progress bar here because it interferes with the book rendering. You can omit the `progress` argument._

```{python get_tor, deco=list()}
tor = ec3.get_data(stations=5051, years=range(1981, 2011), progress=False)
```


## Cleaning the data

Now that we have the data that we need, we can start working with it. First, let's import the libraries we are going to need to manipulate our data. 

```{python import_libs, eval=FALSE, deco=list()}
import numpy as np
import pandas as pd
```

Let's take a look at what our columns are named. 

```{python print_cols, deco=list()}
tor.columns
```

Hmm, some of those are a little cumbersome. Let's rename the columns we are going to use and then select only those columns. 

```{python rename_cols, deco=list()}
tor = tor.rename(columns={'Date/Time': 'Date', 'Max Temp (°C)': 'MaxTemp', 'Min Temp (°C)': 'MinTemp', 'Mean Temp (°C)': 'MeanTemp', 'Total Precip (mm)': 'TotalPrecip'})
tor = tor[['Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

Great. Let's take a look at what we've got. 

```{python tor_head, deco=list()}
tor.head()
```
```{python tor_tail, deco=list()}
tor.tail()
```

Uhh ohh! It looks like we are missing all of our temperature data at the end of our baseline period. We can test this theory by using an enumerator. The following will tell us the index at which all three of our temperature variables of interest are missing for the first time. 
```{python first_missing, deco=list()}
next(i for i, x in enumerate(tor.loc[:, 'MaxTemp':'MeanTemp'].isnull().all(axis=1)) if x) - 1
```

Let's take a look at the context around that index. 

```{python range_missing, deco=list()}
tor[8211:8221]
```

It looks like we're missing data as of July 2003! Something odd has happened. Let's look for a nearby station (within 2 km) that we can use!

```{python find_station, deco=list()}
ec3.find_station(target=5051, dist=range(3))
```

By the looks of it, "Toronto" and "Toronto City" are co-located! Indeed, in 2003 the "Toronto" station was renamed "Toronto City" and re-coded. Since that date, the daily data is available under station code 31688. 

Let's grab that data set as of 2000 and compare with station 5051.

```{python get_tor2, deco=list()}
tor2 = ec3.get_data(stations=31688, years=range(2000, 2011), progress=False)
```

The ECCC data has a standard format, so our column names for `tor2` are going to be the same nasty column names that we had in `tor`. Let's rename them and select the columns we are interested in.

```{python rename_cols2, deco=list()}
tor2 = tor2.rename(columns={'Date/Time': 'Date', 'Max Temp (°C)': 'MaxTemp', 'Min Temp (°C)': 'MinTemp', 'Mean Temp (°C)': 'MeanTemp', 'Total Precip (mm)': 'TotalPrecip'})
tor2 = tor2[['Date', 'MaxTemp', 'MinTemp', 'MeanTemp', 'TotalPrecip']]
```

We can check the head and the tail of this data too. 

```{python tor2_head, deco=list()}
tor2.head()
```
```{python tor2_tail, deco=list()}
tor2.tail()
```

As expected, we're missing data at the beginning of the file (before 2003). We can use a slightly modified version of our iterator from earlier. This time, we'll check for the first row for which any of the data is present. 

```{python first_present, deco=list()}
next(i for i, x in enumerate(tor2.loc[:, 'MaxTemp':'MeanTemp'].notnull().any(axis=1)) if x) - 1
```

We'll take a look at that in context again. 

```{python range_present, deco=list()}
tor2[880:890]
```

It looks like our data starts as of June 04, 2002. 

OK, so we know that we have data at the old station until May 30, 2002. Likewise, we have some data at the new station in May 2002. Let's take a look at the overlap and see if we can merge these files without issue. 

```{python compare1, deco=list()}
tor[(tor.Date >= "2003-06-25") & (tor.Date <= "2003-07-05")]
```
```{python compare2, deco=list()}
tor2[(tor2.Date >= "2003-06-25") & (tor2.Date <= "2003-07-05")]
```

The data on the overlapping days is virtually identical between the two station codes, so we can be confident that merging these two data sets won't lead to a sudden temperature bump. Let's append the relevant section of `tor2` to `tor`.

```{python merge_tors, deco=list()}
tor = tor[tor.Date < "2003-07-01"].append(tor2[tor2.Date >= "2003-07-01"])
```

Now we can make sure that we have a full data set from January 1981 to December 2010. 

```{python merged_head, deco=list()}
tor.head()
```
```{python merged_tail, deco=list()}
tor.tail()
```

Save your new data set to _csv_ by:

```{python save_csv, deco=list()}
tor.to_csv("tor.csv")
```


## Analyzing the data

Now let's take a look at our data. We will require some additional libraries.

```{python import_libs2, eval=FALSE, deco=list()}
import matplotlib as mpl
mpl.rc('font', size = 12)
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import scipy.stats as stats
```

As we mentioned in the introduction, an exposure unit is often the "unit" of our analysis when we want to perform an impact assessment. In this section, we'll calculate two exposure units, starting with a rather simple Boolean (True/False) unit. 

As I write this section, the temperature outside is a sweltering 32 °C (36 with the humidex), so what better exposure unit to consider than "tropical nights", nights with a minimum temperature above 20 °C. We will limit our analysis to the summer months.

This code requires that your "Date" column is recognized as such by Python. This isn't always automatic, so you will likely need to ask **pandas** to convert the column to a `datetime` object. 

```{python fix_date, deco=list()}
tor.Date = pd.to_datetime(tor.Date)
```

### Example: Tropical nights

Now let's start by adding a new column to our table called "TropNight", which will be a True/False value.

```{python calc_trop, deco=list()}
tor['TropNight'] = ((tor.MinTemp > 20) & (tor.Date.dt.month.isin([6, 7, 8])))
tor.head()
```

Since our value is Boolean, we can use that column as an index. Let's double-check to make sure that we got the results that we were expecting.

```{python trop_head, deco=list()}
tor[tor.TropNight].head()
```

In Python, `True` is equal to 1, and `False` is equal to zero. This means that we can easily get an annual data set by grouping our data frame by year and taking the sum of the "TropNight" column for each group. 

```{python agg_trop, deco=list()}
tor_trop = tor[['Date', 'TropNight']].groupby(tor.Date.dt.year).sum()
tor_trop.head()
```

```{python l1f1, deco=list(), results='hold', fig.cap="Total summertime tropical nights at Toronto (1981\u20122010)."}
tor_trop.plot()
```

```{block, type='rmdtip'}
If you don't see a figure after running the code in the previous block, you may need to make an explicit call to `plt.show()`. If you are using Jupyter Notebook, you can add `%matplotlib inline` somewhere after your import statement for **matplotlib** and then the plots will automatically appear in your notebook when they are created. Seeing your plots as you build them is useful to make sure that they look like you might expect them to. Once you have the perfect plot, you can save it to disk using `plt.savefig()`.
```

The grouping operation has changed our table index to the years from 1981 to 2010. We want to keep this information in the table, so we can reset our index like this:

```{python fix_index, deco=list()}
tor_trop.reset_index(level = 0, inplace = True)
tor_trop.head()
```

Now let's make another plot, this time with a trendline. 

```{python l1f2, deco=list(), results='hold', fig.cap="Total summertime tropical nights at Toronto (1981\u20122010), with linear trend line."}
plt.plot(tor_trop.Date, tor_trop.TropNight)
plt.plot(tor_trop.Date, np.polyval(np.polyfit(tor_trop.Date, tor_trop.TropNight, 1), tor_trop.Date))
plt.title("Total summertime tropical nights at Toronto (1981‒2010)")
plt.xlabel("Year")
plt.ylabel("No. of Tropical Nights")
```

### Example: Heating degree days

Let's try for a slightly more complicated exposure unit: Winter heating degree days (HDDs). The HDDs represent the difference between the mean temperature and the base temperature of 18 °C. Since HDDs can't be negative, they are only calculated when the mean temperature is below 18 °C. They are used as an indirect measure of energy demand. 

We can apply an anonymous (lambda) function to our `MinTemp` column to capture the following pseudocode expression:

<center>`return 0 if the mean temperature was above 18, otherwise, return the difference between 18 and the mean temperature`</center>

```{python calc_hdd, deco=list()}
tor['HDD'] = tor.MeanTemp.apply(lambda x: 0 if x >= 18 else 18 - x)
```

We have one more challenge to overcome. A common mistake when performing seasonal analysis on the winter is to simply group December, January, and February by year. This is an easy logical jump to make. Remember, when we refer to winter, we mean the _continuous_ winter season that stretches from the December of the previous year to February of the current year. The easiest way to control for this is to add 1 to the year for any December. Winter 1981/82, for example will then appear with `'Year'` 1982 in the data frame. 

```{python fix_win, deco=list()}
tor['Year'] = tor.Date.apply(lambda x: x.year + 1 if x.month == 12 else x.year)
```

Now we can aggregate our heating degree days. Pay close attention to the code below. First, I use two conditional filters in the square brackets. The first condition (`tor.Year.isin(range(1982, 2011))`) selects all years between 1982 and 2011. We use this filter to drop the two winters that we know are incomplete: winter 1980/81 (which is missing December 1980), and winter 2010/2011, which is really just December 2010 here. (A better solution for this issue would be to download data including December 1980). The second condition (`tor.Date.dt.month.isin([12, 1, 2])`) filters out all months except for the winter months: 12, December; 1, January; and 2, February. After filtering, I select the two columns that are of interest (`'Year'` and `'HDD'`), group them by `'Year'` and aggregate by `sum()`. Finally, I reset the index. 

```{python agg_hdd, deco=list()}
tor_hdd = tor[tor.Year.isin(range(1982, 2011)) & tor.Date.dt.month.isin([12, 1, 2])][['Year', 'HDD']].groupby('Year').sum().reset_index()
tor_hdd.head()
```

```{python l1f3, deco=list(), results='hold', fig.cap="Total wintertime heating degree days (HDD) at Toronto (1981\u20122010), with linear trend line."}
plt.plot(tor_hdd.Year, tor_hdd.HDD)
plt.plot(tor_hdd.Year, np.polyval(np.polyfit(tor_hdd.Year, tor_hdd.HDD, 1), tor_hdd.Year))
plt.title("Winter HDD at Toronto (1981/82‒2009/10)")
plt.xlabel("Year")
plt.ylabel("Heating Degree Days  (HDD)")
```

Let's see if we can detect any trends in our baseline values. For convenience, I am going to create a function that will print the relevant values. You can run these commands directly, but this might save you time when you get to the exercises (Hint!).

```{python def_test_trends, deco=list()}
def test_trends(years, values):
    # Perform linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(years, values)
    # Print the regression statistics
    print("The regression coefficients are", np.round(slope, 3), "for the slope and",
          np.round(intercept, 1), "for the intercept\n")

    # Calculate confidence intervals
    t_crit = stats.t.ppf(0.975, len(years) - 1)
    confidence_interval = t_crit * std_err
    print("The true value of the slope is then", np.round(slope, 3), "+/-",
          np.round(confidence_interval, 3),"\n")

    # Get Pearson's coefficient 
    pearsons_corrcoef, p_corr = stats.pearsonr(years,  values)
    
    # Describe the significance level
    levels = [0.001, 0.01, 0.05, 0.1]
    lvl = [i for i in levels if p_corr < i]
    
    # Print the correlation statistics
    print("The correlation is", np.round(pearsons_corrcoef, 3), "with a p-value of",
          np.round(p_corr, 5), "(not significant)\n" if lvl == [] else
          "(significant at the " + str(min(lvl)) + " level)\n")

    # Calculate and print the R²
    print("The variance in", values.name, "explained by the linear trend is",
          "quantified by the R²: R² =",
          np.round(100 * pearsons_corrcoef**2, 3), '%.\n')
```

Let's use this new function to see some key features of the relationship between the year and the annual summertime tropical nights.

```{python trends_trop, deco=list()}
test_trends(tor_trop.Date, tor_trop.TropNight)
```

At first glance, it appears that there is a positive correlation between tear and tropical nights (i.e. tropical nights are increasing over time). However, the slope of the trend is only 0.037 tropical nights per year, and the confidence intervals are very wide (0.373). The  fact that the confidence intervals cross zero and the large $p$-value tell use that there doesn't appear to be a trend after all. In fact, the $R^2$ tells us that only about 0.15% of the variance in tropical nights can be explained by the linear trend. 

What about heating degree days?

```{python trends_hdd, deco=list()}
test_trends(tor_hdd.Year, tor_hdd.HDD)
```

For heating degree days, we see the opposite: there is an apparent negative trend (i.e. less heating degree days), but we again have confidence intervals that cross zero, a large p-value, and a small $R^2$.

```{block, type='rmdtip'}
You may be asking yourself how it is that we didn't see any trends in tropical nights or in HDD. If the overwhelming scientific consensus is that the world is getting warmer, why didn't we see trends in our exposure units? Remember that the climate system is very noisy. We focused our analysis at a given place over a given period of time. While the general rule is that climate analysis requires _at least_ 30 years of data; even 30 years may be insufficient to detect long-term trends. We also examined two very different exposure units. Tropical nights, for instance, are a relatively extreme index at Toronto. It may be unrealistic to expect that we'd find a statistically significant trend. For a look at how the length of our period affects trends in winter temperatures, see @Anderson_Gough_2017. For another paper on long-term trend anaysis at Toronto, see @Mohsin_Gough_2010.
```

## Exercises (what to submit)

```{block, type='rmdassignment'}
- You have been assigned an exposure unit. Write a brief description of the exposure unit, whether there were any missing or suspicious data, and how these data were treated. [2 marks] 
- Include the time series plots with fitted trend lines for your exposure unit and for the relevant temperature variable, e.g. `MeanTemp` for freezing degree days. [2 marks] 
- Include a table with the summary statistics for your exposure unit and temperature variable. [2 marks] 
- Now that we know that Station 5051 was recoded sometime between 2002 and 2003, future analyses might be quicker if we simply merge the 5051 data up to 2002 and the 31688 data from 2003 onward. Does this change in the join date between the two stations affect your exposure unit analysis results? [2 marks] - Be sure to clearly label your plots and table and include concise figure and table captions. [2 marks]
```
