# Downscaled climate data from PCIC {#pcic}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 10-PCICdata.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
source("R/init_python.R")
source("R/deco_hook.R")
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
mpl.rc('font', size=12)
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as stats
import os
import cftime
import xarray as xr
from netCDF4 import Dataset, num2date
```

We can improve upon our estimate of the impact of climate change on our exposure unit of interest by downscaling GCM data using more sophisticated statistical downscaling techniques.

In this exercise, we will use data that has been downscaled using a bias-corrected constructed analogs method (BCCAQ; see @cannon2015bias). One of the advantages of this method is that we can bias-correct and downscale daily GCM data.

The data was provided by @pcic2014statistically, and is downscaled based on a high-quality gridded observed data set (~10 km grid size) developed by NRCan (ANUSPLIN300). PCIC provides an excellent overview of their downscaling methodology, with citations made to the relevant articles, [here](https://
www.pacificclimate.org/data/statistically-downscaled-climate-scenarios). You should read the entirety of that page before continuing this preface and before starting lab 5.

Figure \@ref(fig:l5f0), below, shows the results of this downscaling process for the Canadian CanESM2 model. Notice that the PCIC product, downscaled to a 10 km grid, is much more closely related to the observed station point data that we downloaded from Environment and Climate Change Canada. Our comparison here still isn't perfect, as we're still comparing a point to a grid cell, but as our grid cell decreases in size, the two series begin to behave more similarly. This definitely looks like a solution to improve the quality of our impact assessment. Read on to learn how to obtain this high-resolution downscaled data directly from PCIC.

```{python make_ds_comp_plot, include=FALSE}
## Downscaled Data

filename = {'tasmax': 'data/pcic_data/tasmax_day_BCCAQv2+ANUSPLIN300_CanESM2_historical+rcp45_r1i1p1_19500101-21001231.nc',
            'tasmin': 'data/pcic_data/tasmin_day_BCCAQv2+ANUSPLIN300_CanESM2_historical+rcp45_r1i1p1_19500101-21001231.nc'}

nc = Dataset(filename['tasmax'])
nc_time = nc.variables['time']
time = num2date(nc_time[:], nc_time.units, nc_time.calendar)
nc_var = np.squeeze(nc.variables['tasmax'][:].data)
nc.close()
df = pd.DataFrame({'Date': time, 'Model': 'CanESM2', 'Variable': 'tasmax', 'Value': nc_var})

nc = Dataset(filename['tasmin'])
nc_var = np.squeeze(nc.variables['tasmin'][:].data)
nc.close()
df = df.append(pd.DataFrame({'Date': time, 'Model': 'CanESM2', 'Variable': 'tasmin', 'Value': nc_var}))   

df = df.pivot_table(
    index=['Date', 'Model'],
    columns='Variable',
    values='Value').reset_index()

df['tas'] = (df.tasmax + df.tasmin)/2
df = df.melt(id_vars=['Date','Model'], value_name="Value")
baseline = df[(df.Date >= cftime.DatetimeNoLeap(1981, 1, 1)) & (df.Date <= cftime.DatetimeNoLeap(2010, 12, 31))]

## Observed Data

T_STN = pd.read_csv("tor.csv", index_col = 0)

## Raw CanESM2 Data

T_hist_CanESM2 = np.genfromtxt('data/CanESM2_tas_19812010_mat.csv', delimiter=',').ravel()

## Plot PDFs of all data for Baseline time period

kernel1 = stats.gaussian_kde(np.asarray(baseline.Value[baseline.Variable=='tas']))
x1 = np.linspace(2*np.asarray(baseline.Value[baseline.Variable=='tas']).min(), 
                 2*np.asarray(baseline.Value[baseline.Variable=='tas']).max(), 1000)
pdf1 = kernel1(x1)

kernel2 = stats.gaussian_kde(np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)])
x2 = np.linspace(2*np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)].min(), 2*np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)].max(), 1000)
pdf2 = kernel2(x2)

kernel3 = stats.gaussian_kde(T_hist_CanESM2)
x3 = np.linspace(2*T_hist_CanESM2.min(), 2*T_hist_CanESM2.max(), 1000)
pdf3 = kernel3(x3)
```

```{python l5f0, echo=FALSE, results='hold', fig.cap="Baseline (1981\u20122100) $T_\\mathrm{mean}$ at Toronto, observed (green), simulated by CanESM2 (yellow), and downscaled by PCIC (blue)."}
fig = plt.figure(figsize=(9,6))

plt.plot(x1, pdf1, color='blue',linewidth=2)
plt.plot(x2, pdf2, color='green',linewidth=2)
plt.plot(x3, pdf3, color='orange',linewidth=2)

plt.plot(np.squeeze(np.mean(np.asarray(T_STN.MeanTemp)[~np.isnan(T_STN.MeanTemp)])*np.ones(x1.size)),
         np.linspace(0,0.045,x1.size),'green',linewidth=2,label='Station (point data)')
plt.plot(np.squeeze(np.mean(np.asarray(baseline.Value[baseline.Variable=='tas']))*np.ones(x1.size)),
         np.linspace(0,0.045,x1.size),'blue',linewidth=2,label='DS CanESM2 (~10 km grid)')
plt.plot(np.squeeze(np.mean(T_hist_CanESM2)*np.ones(x1.size)),
         np.linspace(0,0.045,x1.size),'orange',linewidth=2,label='Raw CanESM2 (~2.8125° grid)')

plt.xlabel('$T_\mathrm{mean}$ ($^{\circ}$C)')
plt.ylabel('Probability Density')
plt.title('$T_\mathrm{mean}$ at Toronto, 1981‒2010')
plt.legend(loc='upper right',fontsize=12)
plt.ylim(0,0.045)
plt.tight_layout()
```

## Retrieving downscaled data from the data portal website

PCIC provides an intuitive point and click map for accessing the downscaled GCM data. In our exercise in Lab 5, we will use six models (CanESM2, CCSM4, CSIRO-Mk3-6-0, GFDLESM2G, inmcm4 and MIROC5). To download this information manually, follow the steps below. You may instead wish to download these data programatically. Skip to the next section for that process. 

1. Open the link to the downscaled GCM data: http://tools.pacificclimate.org/dataportal/downscaled_gcms/map/. A map of North America should appear on your screen.
2. In the bar on the right-hand side of your screen. Select "historical,rcp45".
3. Select one of the downscaled data sets, e.g. CanESM2, r1i1p1, and 'tasmax'. We have to download 'tasmax' and 'tasmin' separately.
4. Set your start date to 1981-01-01, leave the end date as is at 2101-01-01.
5. Now, click on the pencil icon in the top right-hand corner of the map and select the Toronto latitude and longitude (43.67°N and 79.40°W). A small window will pop-up asking if you want to download this data - click 'OK'.
6. Repeat this process 23 times so that you have data for both 'tasmax' and 'tasmin' for all 6 models for RCP 4.5 and RCP 8.5.

```{block, type='rmdtip'}
You can also download whole areas from the PCIC data portal. To do that, use the polygon icon between the pencil and the hand.
```

## Retrieving downscaled data from climatedata.ca

[climatedata.ca](https://climatedata.ca) also provides an intuitive point and click map for accessing the PCIC BCCAQv2 data. To download this information manually, follow the steps below. You may instead wish to download these data programatically. Skip to the next section for that process.

1. Open the link to the downscaled GCM data: https://climatedata.ca/download/. A screen with several drop-down menus and map of Canada should appear on your screen.
2. Select the time frequency of the data you want. We will select daily.
3. Select the variable that you want. We will select maximum temperature (tasmax).
4. Select the location that you want either by searching or by selecting grid boxes on the map. We will select Toronto.
5. Select the data format. We will select netcdf.
6. Finally, enter your email address. You will receive an email with a download link.
7. Note that this [climatedata.ca](https://climatedata.ca) data portal does not allow you to select models or scenarios. Your download will include all available models and scenarios for the PCIC BCCAQv2 data and it will include separate files for each type of daily calendar (e.g. some GCMs include leap years and some do not).
8. Repeat this process one more time so that you have data for both 'tasmax' and 'tasmin' for all models and scenarios.

```{block, type='rmdtip'}
You can download multiple grid cells at a time by holding down the 'Ctrl' key as you click on grid cells on the map.
```

## Retrieving downscaled GCM data programmatically using PAVICS

Power Analytics and Visualization for Climate Science (PAVICS) is a new research platform dedicated to climate analysis and visualization. It bundles data search, analytics and visualization services. PAVICS is developed by [Ouranos](https://www.ouranos.ca/en/ouranos/), [CRIM](https://www.crim.ca/en/) and the [birdhouse community](http://bird-house.github.io/) and been funded by the [CANARIE](https://www.canarie.ca/language/) research software program. We can use Python to access the PAVICS server to download the PCIC BCCAQc2 data.

As always, we first import the necessary libraries.

```{python, deco=list()}
import numpy as np
from datetime import datetime
import xarray as xr
```
Next, we will define the latitude and longitude coordinates that we are interested in using a four-cornered bounding box. Here, I am specifying the latitude and longitude for Toronto. We need to give about a 0.1 degree difference between the corners of the bounding box.

```{python, deco=list()}
# Define bounding box for your location. Use increments of 0.1
BB = dict(lon=[-79.5,-79.4],lat=[43.4, 43.5])
```

The data are hosted on the cloud, so the next step is to specify the server where the data are located.

```{python, deco=list()}
# PAVICS link for BCCAQv2 data
pavics = 'https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/pcic/BCCAQv2/'
```

We are going to loop over different scenarios, variables and models for our download. Let's define lists for each. As above, we will just download data for six GCMs.

```{python, deco=list()}
# lists of variables, scenarios and models
scen = ["rcp85","rcp45"]
var = ['tasmax','tasmin']
model = ["inmcm4", "CanESM2","CCSM4","CSIRO-Mk3-6-0", "GFDL-ESM2G","MIROC5"]
```

Now, we perform the download by looping over each scenario, variable and model using the above lists. Notice that each time we go through the loop, we define a url that specifies the file name we want to download and the filename is constructed using the elements of the lists above.

```{python, deco=list()}
# loop over variables, scenarios and models
for j in range(len(var)):
    for k in range(len(model)):
        for i in range(len(scen)):

            # print progress to screen
            now = datetime.now()
            current_time = now.strftime("%H:%M:%S")
            print('Downloading...',scen[i],var[j],model[k],"Current Time =", current_time)

            # define netcdf filename
            ncfile = pavics+var[j]+'_day_BCCAQv2+ANUSPLIN300_'+model[k]+'_historical+'+scen[i]+'_r1i1p1_19500101-21001231.nc'

            # open file
            grid   = xr.open_dataset(ncfile)

            # Get all data from lat and lon
            lat = grid['lat'][:].values
            lon = grid['lon'][:].values

            # first, check that these statements are true
            assert lat.shape[0] == grid.variables[var[j]].shape[1], 'Second dim is lat here'
            assert lon.shape[0] == grid.variables[var[j]].shape[2], 'Third dim is lon here'

            # nonzero returns a tuple of idx per dimension
            # we're unpacking the tuple here so we can lookup max and min
            (latidx,) = np.logical_and(lat >= BB['lat'][0], lat < BB['lat'][1]).nonzero()
            (lonidx,) = np.logical_and(lon >= BB['lon'][0], lon < BB['lon'][1]).nonzero()

            # times to extract (data starts in 1950)
            start = 31*365 # only want data from 1981 onwards
            end = grid.variables[var[j]].shape[0] + 1 # to the end

            # Get the temperature data over bounding box
            tmp = grid[var[j]][start:end,latidx.min():latidx.max(),lonidx.min():lonidx.max()]

            # save to netcdf
            tmp.to_netcdf(var[j] +'_'+ scen[i] +'_' + model[k]+ ".nc")
```

This code downloads the data into your working directory.

## Retrieving downscaled GCM data from PCIC programmatically

The Pacific Climate Impacts Consortium (PCIC) data portal can also be accessed programmatically via Python or your preferred programming language. If you performed any manual downloading, above, you may have noticed that the data portal webpage returns some interesting data set URLs. We can use the same specific URL syntax to automate our downloading. We can do this in Python using the code, below.

As always, we first import the necessary libraries.

```{python, deco=list()}
import datetime as dt
import json
import numpy as np
import os
import re
import requests
from netCDF4 import Dataset, date2index, num2date, date2num
from tqdm import tqdm
```

Here we will use a similar `download` function as the one that we used in Lab 2 (Sec. \@ref(lab2)) so that we can keep track of the files as they download. 

```{python, deco=list()}
# Adapted from: https://stackoverflow.com/a/37573701
def download(url, filename):
    print("Downloading ", filename)
    r = requests.get(url, stream=True)
    total_size, block_size = int(r.headers.get('content-length', 0)), 1024
    with open(filename, 'wb') as f:
        for data in tqdm(r.iter_content(block_size),
                         total=total_size//block_size,
                         unit='KiB', unit_scale=True):
            f.write(data)
    if total_size != 0 and os.path.getsize(filename) != total_size:
        print("Downloaded size does not match expected size!\n",
              "FYI, the status code was ", r.status_code)
```


PCIC maintains a JSON-formatted catalogue of file IDs and URLs, which we can retrieve in Python directly. 

```{python, deco=list()}
with requests.get("http://tools.pacificclimate.org/dataportal/downscaled_gcms/catalog/catalog.json") as r:
    data = json.loads(r.content)
```

We cannot access the data entries with an integer index, but rather need to access the data using the keys. We can print the fill list of keys in the data object using `data.keys()`. This is a very long list, so I will omit it here. Let's say that we are interested in accessing the data set with the key `tasmax_day_BCCAQv2_CanESM2_historical-rcp26_r1i1p1_19500101-21001231_Canada`. We can find the URL for this like this: 

```{python, deco=list()}
url = data['tasmax_day_BCCAQv2_CanESM2_historical-rcp26_r1i1p1_19500101-21001231_Canada']
url
```

Since this is a netCDF file, we can access its dimensions directly, just like we have done for files stored on the disk.

```{python, deco=list()}
nc = Dataset(url)
nc_time = nc.variables['time']
nc_lat = nc.variables['lat'][:].data
nc_lon = nc.variables['lon'][:].data
```

Now we can find the relevant indices. Here I am using `d` to represent day, `x` to represent longitude, and `y` to represent latitude.

```{block, type='rmdcomment'}
While we are only really looking for a single point in our exercises, the code in this chapter requests a box of zero length and zero width. If, in the future, you plan to peform a regional study on a larger spatial scale just set your xmin (left), xmax (right), ymin (bottom) and ymax (top) to capture all of the cells that you are interested in. 
```

```{python, deco=list()}
target = {"ymin": 43.67, "xmin": -79.40, "ymax": 43.67, "xmax": -79.40}
# FIXME: As of the time of writing, using non-zero dmin indeces causes issues
#dmin = date2index(dt.datetime(1981, 1, 1, 12, 0, 0), nc_time, select="nearest")
dmin = 0
dmax = len(nc_time)
ymin = np.argmin(np.abs(nc_lat - target['ymin']))
xmin = np.argmin(np.abs(nc_lon - target['xmin']))
ymax = np.argmin(np.abs(nc_lat - target['ymax']))
xmax = np.argmin(np.abs(nc_lon - target['xmax']))

print("Our start date is", num2date(nc_time[dmin], nc_time.units, nc_time.calendar),
      "\nWe need data starting at time T={} (of {}), with Y from {} to {} and X from {} to {}".format(dmin, dmax, ymin, ymax, xmin, xmax))
nc.close()
```

Now that we know the specific dimensions that we need to download, we can find the keys for the full set of models that we desire, for both RCP4.5 and RCP8.5. We will use a special search pattern called a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) for this. 

```{python, deco=list()}
models = ["CanESM2", "CCSM4", "CSIRO-Mk3-6-0", "GFDL-ESM2G", "inmcm4", "MIROC5"]
```

Here we are collapsing our list of models using '`|`', which operates in our regular expression to mean "OR". The same goes for our variables and scenarios "`tasm(in|ax)`" will match both variables, and "`rcp(4|8)5`" will match both "rcp45" and "rcp85".


```{python, deco=list()}
pattern = r'tasm(in|ax).*BCCAQv2.*(' + '|'.join(models) + r').*-rcp(4|8)5_r1i1p1_.*'
pattern
```

Now we can pull out our desired keys

```{python, deco=list()}
keys = [i for i in data.keys() if re.match(pattern, i)]
"\n".join(keys)
```

You might expect that you can access the data in the PCIC netCDF files directly, just like we have done for the dimensions. This is usually true of netCDF files but at the time of writing, trying to access the files directly was [giving unexpected results](https://github.com/pacificclimate/pdp/issues/94). However, we can download the required files to disk and access them locally using the `download()` function that we defined earlier. Before we download the files, however, we need to know the URL and filename of each file. 


```{python, deco=list()}
urls = [data[key] for key in keys]
filenames = [os.path.basename(url) for url in urls]

# FIXME, when the dmin error is fixed, you might want to change the date in the filename
#filenames = [re.sub('19500101', '19810101', i) for i in filenames]
```

The data portal [documentation](https://web.archive.org/web/20190624031923/https://data.pacificclimate.org/portal/docs/raster.html#power-user-howto) explains that we need to format our request URL with a valid [OPeNDAP](http://opendap.org/) specification. In our case, the URL will look like this:

<center> `https://data.pacificclimate.org/data/downscaled_gcms/data/<datasetid>.nc?<variable>[<dmin:dmax>][<xmin:xmax>][<ymin:ymax>]` </center>

We need to fill in the `dataset_id` with the URL from the catalogue that we downloaded earlier. We will complete the DAP specification with either "tasmax" or "tasmin" and `[<dmin:dmax>][<xmin:xmax>][<ymin:ymax>]`. You may notice that the requested URL has the _.nc_ extension twice. This is because we are asking the server to access the netCDF file and to return the result as a netCDF file. Other options include _.csv_ and ASCII gridded data, but these are metadata-poor formats and are not recommended.

```{python, deco=list()}
url_suffix = "[{}:{}][{}:{}][{}:{}]".format(dmin, dmax, ymin, ymax, xmin, xmax)
urls = [url + ".nc?" + re.search(r'tasm(in|ax)', url)[0] + url_suffix for url in urls]
```

You may wish to confirm that your filenames and URLs match, e.g.

```{python, deco=list()}
filenames[0]
```

```{python, deco=list()}
urls[0]
```

Now you can proceed to download these files by looping over the `filenames` and `urls` objects. Note that, at the time of writing, the download speeds were _very_ slow (< 500 bits/second or about 0.0625 kb/s). In addition, it seems that subsetting in the time dimension causes downloads to fail after about 3kb are downloaded, so you may in fact need to download the entire time series. Given these current issues, you may prefer to download the data from [this book's files](https://claut.gitlab.io/man_ccia/data/pcic_data/index.html).

```{python, eval=FALSE, deco=list()}
for i in range(len(urls)):
    download(urls[i], filenames[i])
```

Now that you have the data, we can proceed to analyze it. In Lab 5 (Sec. \@ref(lab5)), we will compare the downscaled GCM data to our observed data and re-assess our exposure unit based on these new data.
