# Lab exercise 3: Navigating across three dimensions of data {#lab3}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 07-Lab3.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
source("R/init_python.R")
source("R/deco_hook.R")
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
import seaborn as sns
mpl.use("agg") # Needed for the book only
mpl.rc('font', size = 12)
from matplotlib import pyplot as plt
plt.switch_backend('agg') # Might be overkill
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', 10)
import datetime as dt
import re
from netCDF4 import Dataset, date2index, num2date, date2num
```


Now that we have acquired our GCM output data (Lab 2, Sec. \@ref(lab2)), we need to know how to access it and begin to analyze it. In this lab, you will find a spatial and temporal subset of your GCM data matching your observed climate baseline, and will begin to understand the future climate using change factors. By the end of this lab, you will have data from more than 40 GCMs. In Lab 4 (Sec. \@ref(modelselection) and \@ref(lab4)), we will see how we can narrow-down our selection of the models that will inform our climate change impact assessment. We are going to begin by exploring some of the attributes of three dimensional data contained in the downloaded netCDF files. It is strongly recommended that you have a browser tab open with the **netCDF4** library [documentation](http://unidata.github.io/netcdf4-python/) for reference.

We begin, as always, by importing the libraries that we will need.

```{python, eval=FALSE, deco=list()}
import datetime as dt
import numpy as np
import pandas as pd
import re
from matplotlib import pyplot as plt
from netCDF4 import Dataset, date2index, num2date, date2num
```

Now, create a list of the netCDF files that we will examine. We have created some miniature netCDF files for use in this book, but you should substitute the directories and filenames below with paths leading to the netCDF files that you downloaded in Lab 2.  

```{python, deco=list()}
past = "data/tas_Amon_CanESM2_historical_r1i1p1_198001-200512_mini.nc"
future = ["data/tas_Amon_CanESM2_rcp26_r1i1p1_200601-204012_mini.nc", "data/tas_Amon_CanESM2_rcp45_r1i1p1_200601-204012_mini.nc", "data/tas_Amon_CanESM2_rcp85_r1i1p1_200601-204012_mini.nc"]
```

## The past

We will begin by looking at our "historical" netCDF file. We can load this file into memory using the `Dataset()` function that we imported from **netCDF4**.

```{python, deco=list()}
nc = Dataset(past)
```

First, we recommend that you get familiar with your data set. For instance, what are the dimensions of your data? _Remember, yours should look much different than the output, below_.

```{python, deco=list()}
print(nc.dimensions)
```

Notice that we have three-dimensional data, stored along X (longitude), Y (latitude), and T (time). Some netCDF files also have a Z dimension (e.g. height). There is no hard limit to the number of dimensions a netCDF file can have, though [it is suggested](https://www.unidata.ucar.edu/software/netcdf/docs/group__dimensions.html) not to exceed 1024. Rest assured, it is unlikely that you will encounter a 1024-D file in the wild. To get a list of the variables stored in your netCDF file, query the `variables` attribute of your loaded dataset. 

```{python, deco=list()}
print(nc.variables)
```

You can query a specific variable as well, e.g. `time`.

```{python, deco=list()}
print(nc.variables['time'])
```

The above shows us some interesting characteristics of our variable, such as units, current shape, and whether it is an unlimited variable or not. 

We can store the variable in a Python object, which will help us to get the its values. 

```{python, deco=list()}
nc_time = nc.variables['time']
print(nc_time[0:10])
```

### Get subset indices {#getindices}

The time variable isn't very easy for us to read, as it is stored as "Days since January 1, 1850". Luckily, there are a few helper functions to help us get our period of interest: 1981 to 2010. 

```{python, deco=list()}
time_start = date2index(dt.datetime(1981, 1, 1), nc_time, select="nearest")
time_end = date2index(dt.datetime(2010, 12, 31), nc_time, select="nearest")
```

The observant among you may be asking how we can get data up to 2010 in a file that only contains data to 2005. We can't! We have selected the index that contains data _nearest_ to our desired end date. If we check the true date, we will see that we only have data to 2005. 

```{python, deco=list()}
print(num2date(nc_time[time_end], units=nc_time.units, calendar=nc_time.calendar))
```

Most of the CMIP5 climate models force data with the RCPs as of January 2006. This means that we will need to be conscious of the fact that the last five years of our baseline will be forced with the scenario boundary conditions, not the historical boundary conditions. We will deal with this in upcoming labs.

We now know the slice of T that we need. How about X and Y? We will select a single grid cell, based on our station target. First, I will define that target. I have chosen to use a Python dictionary, because I have a tendency to get my "lats" and my "lons" confused. You can use a regular list if you prefer. 

```{python, deco=list()}
# Toronto station coords
target = {"lat": 43.67, "lon": -79.40}
```

We are used to using negatives for western longitudes (i.e. 180°W to 180°E), but the CMIP5 models use longitudes from 0°E to 360°E instead. We'll need to convert our longitude.

```{python, deco=list()}
if target['lon'] < 0:
    target['lon'] = 360 + target['lon']

print(target['lon'])
```

Now we can get the data for our latitude and longitude variables. Notice the syntax below. We query the variable using `nc.variables.get()`, then we choose the full range of the data using `[:]`, which returns a masked array. We pull the values out of that array with `.data`. 

```{python, deco=list()}
nc_lat = nc.variables['lat'][:].data
nc_lon = nc.variables['lon'][:].data
```

Now we can find the latitude and longitude cell that is closest to our target. 

```{python, deco=list()}
lat_cell = np.argmin(np.abs(nc_lat - target['lat']))
lon_cell = np.argmin(np.abs(nc_lon - target['lon']))
```

```{block, type='rmdtip'}
The minimal netCDF files that we've created for this book do not include "bounds" variables, however, these can be useful. It is not always clear what the "latitude" and "longitude" of a cell refers to: is the centre point? Is it the top-left corner of the grid box? This is where the bounds variables are really useful. The latitude bounds provide the latitude of the top and bottom of each grid box. The longitude bounds provide the right and left sides of the box. Using the bounds can be a safer way to identify the grid box that we are interested in. Depending on the model you have selected, look for a variable called `lat_bnds` or `lat_bounds`.

<div class="decocode"><div style="background-color:#366994"><p><span style="font-size:90%;color:#ffffff"><i class="fab fa-python"></i> <b>Python</b></span></p>
    nc_lat_bnds = nc.variables["lat_bnds"][:].data
    lat_cell = np.where((nc_lat_bnds[:,0] < target['lat']) & (nc_lat_bnds[:,1] > target['lat']))[0][0]
    nc_lon_bnds = nc.variables["lon_bnds"][:].data
    lon_cell = np.where((nc_lon_bnds[:,0] < target['lon']) & (nc_lon_bnds[:,1] > target['lon']))[0][0]
</div></div>
```

### Getting the data

Now that we know the indices of X, Y, and T that we are interested in, we can pull these out of the netCDF file. 

```{python, deco=list()}
dat = nc.variables['tas'][time_start:(time_end + 1), lat_cell, lon_cell]
print(dat[0:10])
```

You may have noticed temperatures in the 200s. Our data is in Kelvin. To convert to Celsius, we can subtract 273.15 from every value. For convenience, I will also pull the values out of the masked array using `.data`.

```{python, deco=list()}
dat = np.subtract(dat.data, 273.15)
```

Let's make sure that we got the right amount of time and data: 12 months time 25 years for 300 data points.

```{python, deco=list()}
print(time_end - time_start + 1)
```

```{python, deco=list()}
print(len(dat))
```

We can convert our time deltas into dates as follows:

```{python, deco=list()}
dates = num2date(nc_time[time_start:time_end + 1], units=nc_time.units, calendar=nc_time.calendar)
```

Now create a Pandas data frame.

```{python, deco=list()}
df = pd.DataFrame({'Date': dates, 'MeanTemp': dat, 'Experiment': 'historical'})
print(df.head())
```

Even though we are using monthly data, the date column is still formatted with a full YYYY-MM-DD hh:mm:ss date! The date and time represents approximately the middle of the month (probably from averaging the values from a smaller time scale). In addition, the calendar used in CanESM2 is a "365 day" calendar, which ignores leap years. The `num2date` function that we used above, converts the values of the time dimension into a special `cftime.DatetimeNoLeap` object. Unfortunately, this date format does not react intuitively in plots. Given that we are using monthly data, it doesn't matter to us whether there were 28 or 29 days in February of any given year. As such, let's convert the `cftime.DatetimeNoLeap` object to a regular Python `datetime.date` with a YYYY-MM-DD format. 

```{python, deco=list()}
df['Date'] = [dt.date(i.year, i.month, i.day) for i in df.Date]
```

Let's see what our data frame looks like.

```{python, deco=list()}
print(df.head())
```

```{python, deco=list()}
print(df.tail())
```

We can plot the results of our analysis up to this point.

```{python, deco=list()}
plt.plot(df.Date, df.MeanTemp)
plt.title(r'CanESM2 Monthly $T_{mean}$ at Toronto (1981‒2005)')
plt.xlabel("Year")
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l3f1.png')
plt.clf()
```

```{r l3f1, echo=FALSE, fig.cap="Monthly $T_{mean}$ at Toronto (1981\u20122005), as simulated by CanESM2."}
knitr::include_graphics("l3f1.png", dpi = NA)
```

It is always a good idea to close our netCDF file when we are done with it. 
```{python, deco=list()}
nc.close()
```

## The future

I will condense the above process into a quick `for` loop across the three netCDF files that contain projections. In your case, you may have two or even four such files, depending on the number of scenarios for which simulation data is available for your model. Also note that I will only extract data to 2040, but you should use 2100 as your upper date. A reminder of the contents of `future`:

```{python, deco=list()}
print(future)
```

This loop will process the data from each file in the same way that we processed the historical data, and will create one data frame containing all of our data. 

```{python, deco=list()}
for file in future:
    nc = Dataset(file)
    nc_time = nc.variables["time"]
    time_start = date2index(dt.datetime(2006, 1, 1), nc_time, select="nearest")
    time_end = date2index(dt.datetime(2040, 12, 31), nc_time, select="nearest")
    lat_cell = np.argmin(np.abs(nc_lat - target['lat']))
    lon_cell = np.argmin(np.abs(nc_lon - target['lon']))
    dat = nc.variables['tas'][time_start:(time_end + 1), lat_cell, lon_cell]
    dat = np.subtract(dat.data, 273.15)
    dates = num2date(nc_time[time_start:time_end + 1], units=nc_time.units, calendar=nc_time.calendar)
    df2 = pd.DataFrame({'Date': dates, 'MeanTemp': dat, 'Experiment': re.findall(r'rcp[24568]{2}', file)[0]})
    df2['Date'] = [dt.date(i.year, i.month, i.day) for i in df2.Date]
    df = df.append(df2, sort = True)
    nc.close()
```

The table that we have created above, is a "long" data frame. A long data frame is what is referred to as a "tidy" data frame in the R universe. Tidy data is characterized by a few simple principles: each column is a variable, each row is an observation, each table contains a unique type of observational unit.

A long layout table is very useful for many operations such as grouping and subsetting, but can be a challenge to plot. For instance: 

```{python, deco=list()}
df.plot()
```
```{python, include = FALSE}
plt.savefig('l3f2.png')
plt.clf()
```

```{r l3f2, echo=FALSE, fig.cap="Somewhere in here is the CanESM2 Monthly $T_{mean}$ at Toronto (2006‒2040) using three RCP scenarios, but it looks like we need some more sophisticated code to plot this!."}
knitr::include_graphics("l3f2.png", dpi = NA)
```

That doesn't look right. We can manually fix the above by filtering for each projection and adding the line manually. 

```{python, deco=list()}
plt.plot(df.Date[df.Experiment == "rcp26"], df.MeanTemp[df.Experiment == "rcp26"], 'y-', alpha=0.7, label = 'RCP 2.6')
plt.plot(df.Date[df.Experiment == "rcp45"], df.MeanTemp[df.Experiment == "rcp45"], 'y-', alpha=0.7, label = 'RCP 4.5')
plt.plot(df.Date[df.Experiment == "rcp85"], df.MeanTemp[df.Experiment == "rcp85"], 'r-', alpha=0.7, label = 'RCP 8.5')
plt.legend(loc='lower right')
plt.title(r'CanESM2 Monthly $T_{mean}$ at Toronto (2006‒2040)')
plt.xlabel("Year")
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l3f3.png')
plt.clf()
```

```{r l3f3, echo=FALSE, fig.cap="Monthly $T_{mean}$ at Toronto (2006\u20122040), as simulated by CanESM2 using three RCP scenarios."}
knitr::include_graphics("l3f3.png", dpi = NA)
```

What if we want to include the "historical" data? The **seaborn** library can also make this process a little easier. Run the following if you have installed **seaborn**. 

```{python, deco=list()}
import seaborn as sns
fg = sns.FacetGrid(hue="Experiment", data=df, aspect=1.61)
fg.map(plt.plot, "Date", "MeanTemp").add_legend()
plt.title(r'CanESM2 Monthly $T_{mean}$ at Toronto (1981‒2040)')
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l3f4.png')
plt.clf()
```

```{r l3f4, echo=FALSE, fig.cap="Historical and future monthly $T_{mean}$ at Toronto (1981\u20122040), as simulated by CanESM2. Future projections are forced using three RCP scenarios."}
knitr::include_graphics("l3f4.png", dpi = NA)
```

A long data frame is computationally efficient, but isn't as intuitive to navigate by the human eye. As such, if we want to include a table in a report or publication, the wide layout is usually more print-friendly. You can create a wide layout table using `df.pivot` like this:

```{python, deco=list()}
df_wide = df.pivot(index='Date', columns='Experiment')['MeanTemp']
print(df_wide.head())
```

```{python, deco=list()}
print(df_wide.tail())
```

This is also easier to plot with the `plot` method built into **pandas**. 

```{python, deco=list()}
df_wide.plot(title = r'CanESM2 Monthly $T_{mean}$ at Toronto (1981‒2040)')
plt.ylabel("Mean Temperature (°C)")
```
```{python, include = FALSE}
plt.savefig('l3f5.png')
plt.clf()
```

```{r l3f5, echo=FALSE, fig.cap="Historical and future monthly $T_{mean}$ at Toronto (1981\u20122040), as simulated by CanESM2. Future projections are forced using three RCP scenarios."}
knitr::include_graphics("l3f5.png", dpi = NA)
```

## Change Factors

In our next lab, we are going to use change factors to describe changes in our baseline climate. The change factor method (sometimes called $\Delta T$) calculates the projected temperature by adding the observed, baseline temperature time series to the change in the temperature projected by a CGM integration (typically using monthly GCM data). This process removes the bias in the GCM baseline climatology, and represents a simplified form of downscaling. Change factors are described by the following equation:  

<center>$$T_{\mathrm{FUTURE}} = T_{\mathrm{OBSERVED}} + \Delta T_{\mathrm{GCM}}$$</center>

The first step for a change factor analysis is to calculate the change projected by a GCM over its baseline period. To do so, we will use 30-year "tridecades", including our baseline (1981--2010), and three common climate change tridecades: the 2020s (2011--2040), the 2050s (2041--2070), and the 2080s (2071--2100). In this case, $T_{\mathrm{OBSERVED}}$ is the observed 30-year average mean temperature over the baseline period, and $\Delta T_{\mathrm{GCM}}$ is the difference between the model-projected 30-year average mean temperature for the baseline period and the model-projected 30-year average mean temperature for each of the tridecades. In your research, you may opt to use a seasonal or monthly average instead of the annual average presented here.

Let's begin by filtering our "long" data frame to our areas of interest. I will only provide examples for the baseline and the 2020s, but you should complete this work for the 2050s and 2080s as well. 

```{python, deco=list()}
df_base = df[(df.Date >= dt.date(1981, 1, 1)) & (df.Date <= dt.date(2010, 12, 31))]
df_2020s = df[(df.Date >= dt.date(2011, 1, 1)) & (df.Date <= dt.date(2040, 12, 31))]
```

Recall from \@ref(getindices) that our baseline is going to be partially forced by each climate change scenario. To minimize the (albeit small) impact of the scenario forcing on the last five years of the baseline, we will calculate the baseline forced with each scenario, and use the average of those values.

```{python, deco=list()}
bsln26 = df_base[df_base.Experiment.isin(["historical", "rcp26"])].MeanTemp.mean()
bsln45 = df_base[df_base.Experiment.isin(["historical", "rcp45"])].MeanTemp.mean()
bsln85 = df_base[df_base.Experiment.isin(["historical", "rcp85"])].MeanTemp.mean()
bsln = np.mean([bsln26, bsln45, bsln85])
```

Now we can calculate the change factor (or anomaly) for each of our tridecadal periods.

```{python, deco=list()}
cf2020s_rcp26 = df_2020s[df_2020s.Experiment == "rcp26"].MeanTemp.mean() - bsln
cf2020s_rcp45 = df_2020s[df_2020s.Experiment == "rcp45"].MeanTemp.mean() - bsln
cf2020s_rcp85 = df_2020s[df_2020s.Experiment == "rcp85"].MeanTemp.mean() - bsln
```

We can summarize these data in a table like so:

```{python, deco=list()}
print(pd.DataFrame({"Model": np.repeat("CanESM2", 3),
                    "Ensemble": np.repeat("r1i1p1", 3),
                    "Scenario": ["RCP2.6", "RCP4.5", "RCP8.5"],
                    "Baseline (°C)": np.repeat([bsln], 3),
                    "2020s": [cf2020s_rcp26, cf2020s_rcp45, cf2020s_rcp85],
                    "2050s": np.repeat([''], 3),
                    "2080s": np.repeat([''], 3)}))
```

## Exercises (what to submit)

```{block, type='rmdassignment'}
- Complete a table similar to the one above for the model that you have used throughout this lab. [2 marks]
- The table, below, provides the baseline mean temperature (°C) and 2020s anomaly for RCP 2.6 and RCP 4.5 for all five available ensemble members of CanESM2. In some cases, the anomaly for the RCP 2.6 scenario is _higher_ than the RCP 4.5 scenario. What are some possible explanations for this? _Hint: two key terms to consider are "pathway" and "ensemble member"._ [2 marks]

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-mcqj{font-weight:bold;border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-mcqj">Ensemble</th>
    <th class="tg-mcqj">Scenario</th>
    <th class="tg-mcqj">Baseline</th>
    <th class="tg-mcqj">2011-2040</th>
  </tr>
  <tr>
    <td class="tg-73oq" rowspan="2">r1i1p1</td>
    <td class="tg-73oq">RCP2.6</td>
    <td class="tg-73oq">11.9</td>
    <td class="tg-73oq">1.6</td>
  </tr>
  <tr>
    <td class="tg-73oq">RCP4.5</td>
    <td class="tg-73oq">11.9</td>
    <td class="tg-73oq">1.5</td>
  </tr>
  <tr>
    <td class="tg-73oq" rowspan="2">r2i1p1</td>
    <td class="tg-73oq">RCP2.6</td>
    <td class="tg-73oq">11.6</td>
    <td class="tg-73oq">1.8</td>
  </tr>
  <tr>
    <td class="tg-73oq">RCP4.5</td>
    <td class="tg-73oq">11.6</td>
    <td class="tg-73oq">2.1</td>
  </tr>
  <tr>
    <td class="tg-73oq" rowspan="2">r3i1p1</td>
    <td class="tg-73oq">RCP2.6</td>
    <td class="tg-73oq">11.9</td>
    <td class="tg-73oq">1.9</td>
  </tr>
  <tr>
    <td class="tg-73oq">RCP4.5</td>
    <td class="tg-73oq">11.9</td>
    <td class="tg-73oq">1.5</td>
  </tr>
  <tr>
    <td class="tg-73oq" rowspan="2">r4i1p1</td>
    <td class="tg-73oq">RCP2.6</td>
    <td class="tg-73oq">11.9</td>
    <td class="tg-73oq">1.5</td>
  </tr>
  <tr>
    <td class="tg-73oq">RCP4.5</td>
    <td class="tg-73oq">11.9</td>
    <td class="tg-73oq">1.6</td>
  </tr>
  <tr>
    <td class="tg-73oq" rowspan="2">r5i1p1</td>
    <td class="tg-73oq">RCP2.6</td>
    <td class="tg-73oq">11.8</td>
    <td class="tg-73oq">1.6</td>
  </tr>
  <tr>
    <td class="tg-73oq">RCP4.5</td>
    <td class="tg-73oq">11.8</td>
    <td class="tg-73oq">1.8</td>
  </tr>
</table>

- Using [_Conjuntool_](https://shiny.conr.ca/conjuntool/), create a _.csv_ file for all of the models that are available for ensemble r1i1p1. [2 marks]
  - Click on the "GCM Anomalies" tab.
  - Under "1: Def. params", enter the coordinates for Toronto in the "Location" box.
  - Under "2: Add. filters", ensure that only r1i1p1 is selected
  - Under "4: Go!", click the "Process!" button and wait for the results.
  - Under "5. Add. Opts.", select the "Baseline Averages" and "Calculate Anomalies" boxes. 
  - Click the "Download Table" option below the table.
- Compare the output of the models from the _Conjuntool_. Are the model projections largely similar, or different? What might explain some of these similarities or differences. [2 marks]
- Provide a brief summary of the _Conjuntool_ output. Which model projects the largest change in temperatures? Which 
projects the smallest? What is the overall average baseline and change factor for each period across all models? [2 marks]
```

