# Data dimensions and the netCDF file format {#dimensions}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 06-data_dimensions.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(reticulate)
conda <- ifelse(file.exists("/opt/conda/bin/conda"), "/opt/conda/bin/conda", "/data/home-ext/miniconda3/bin/conda")
use_condaenv("man_ccia", conda = conda, required = TRUE)
```

```{bash, include=FALSE}
curl -L -o ncdump.py https://gitlab.com/claut/man_ccia/snippets/1734337/raw 
```

## The dimensions of data

In Section \@ref(modelling), we discussed climate models in terms of the dimensions of the model. We can also express the dimensions of our data in analogous terms. When we refer to a dimension, we are referring to some characteristic of our data that will influence the value of a variable of interest. A dimension could be temporal (e.g. hour of the day), spatial (e.g. latitude), or even categorical (treatment type). The most important quality of the dimension is that it should be composed of unique, non-overlapping values. In the following subsections, we will provide a brief overview of dimensionality, and provide visual examples of 0-, 1-, 2-, 3-, and 4-D data. ^[Some of the code used in the graphics in this chapter was modified from the [SWAN wave model](http://www.texample.net/tikz/examples/swan-wave-model/) and [Using signed distance functions to embed contours in discrete grids](http://www.texample.net/tikz/examples/contours-grids/) TikZ examples.] 

### Zero-dimensional data

An obvious starting point (no pun intended) of our explanation of data dimensions is 0-D data. Zero-dimensional data refers to a single point of information. This point may reflect some measure or value at a specific location, time, and/or set of conditions. We cannot ascertain anything about how this data might change over time or space. Figure \@ref(fig:0d) provides an example of 0-D data. 

```{r 0d, fig.cap="0-D (point) data", fig.align="center"}
knitr::include_graphics("img/0d.png")
```

### One-dimensional data

One dimensional data provides us with an additional piece of information: how does our variable of interest vary over some dimension. We can consider each column of the observed data that we collected in Lab 1 (Sec. \@ref(lab1)) as a good example of 1-D data. We are interested in determining how our variable of interest (`MeanTemp`, for instance) varies over time. Don't be confused by the table format that we used to layout our observed data. The `Date` column in our data set amounts to metadata describing the time (T) dimension. Figure \@ref(fig:1d) provides a visual approximation of one-dimensional data.

```{r 1d, fig.cap="1-D (vector) data", out.width='40%', fig.align="center"}
knitr::include_graphics("img/1d.png")
```

### Two-dimensional data

You are probably most accustomed to working with two-dimensional data sets. In 2-D (rectangular) data, we have a mix of two dimensions of variation. In the case of the data that we collected in Lab 1, we can consider our two dimensions as time (T) and variable. Another common pair of dimensions are latitude and longitude. Recall the "Overlay Map" that you generated on _Conjuntool_ in Lab 2 (Sec. \@ref(lab2)). This is a great example of 2-D data: we include latitude, and longitude, but removed the time dimension by averaging. Refer to Figure \@ref(fig:2d) for a visualization of 2-D data.

```{r 2d, fig.cap="2-D (rectangular) data", out.width='55%', fig.align="center"}
knitr::include_graphics("img/2d.png")
```

### Three-dimensional data

As you have probably ascertained by now, 3-D data refers to a data set that varies over three dimensions. This is most commonly the expression of a set of gridded observations, varying across two spatial axes (X, e.g. longitude and Y, e.g. latitude) and some third dimension such as time (T) or height (Z). The CMIP5 data that you acquired in Lab 2 is an example of three-dimensional data with dimensions (X, Y, and T). When thinking of three-dimensional objects, your mind probably jumps to a cube, which is probably the first 3-D shape any of us learned about in grade school. It is more useful, however, to consider 3-D data as a stack of papers on your desk. Each sheet is a table of X by Y and represents one observation of dimension Z. Have a look at Figure \@ref(fig:3d) to visualize this.  

```{r 3d, fig.cap="3-D (gridded) data", out.width='70%', fig.align="center"}
knitr::include_graphics("img/3d.png")
```

### Data with four or more dimensions

While we won't work with data of four or more dimensions in the exercises in this manual, it is important to note that there is by no means a limit of 3 dimensions to any data set. As we add more dimensions, our data becomes harder for us to visualize. A four-dimensional system, for instance, may include time (T), latitude (Y), longitude (X), and altitude (Z). In Figure \@ref(fig:4d), for instance, we've modelled a four-dimensional data set, where each layer of the Z dimension shown in Figure \@ref(fig:3d) contains several additional layers in an altitude (H) dimension. As we add more dimensions, the shape of our data becomes more and more theoretical, and beyond my capabilities to illustrate! 

```{r 4d, fig.cap="4-D (layered grid) data", out.width='70%', fig.align="center"}
knitr::include_graphics("img/4d.png")
```

## The netCDF file format

Just as higher-dimensioned data becomes difficult for me to illustrate, so too does it become difficult to store in typical text-based file formats like _.csv_ files. Some of the [Global Historical Climatology Network's gridded products](https://www.ncdc.noaa.gov/temp-and-precip/ghcn-gridded-products/), for instance, use an ASCII-formatted grid that looks something like the following: 

```{r, echo=FALSE}
for (month in 1:3) {
  cat(month, "\t1880\n")
  write.table(format(round(matrix(rnorm(20, 100, 40), nrow = 4, ncol = 5)), justify="right"),
            row.names = FALSE, col.names = FALSE, quote = FALSE)
}
```

The above isn't too unwieldy in my example of a 4 by 5 grid with three layers. The actual data set, however, is on a 36 by 72 grid with over 100 years of data. That required me to do a little bit of [cheeky parsing](https://gitlab.com/ConorIA/claut/blob/master/R/parse_ASCII_grid.R) to read the data into R for a previous analysis. Another disadvantage of text-based data is that it usually requires a separate README file with metadata such as the latitude and longitude coordinates for each cell, and basic facts about the data such as the units, method of collection, licensing and use, etcetera. 

Finding solutions to these challenges falls within the mandate of the [Unidata program](https://www.unidata.ucar.edu/), managed by the [University Corporation for Atmospheric Research](https://www.ucar.edu/). Unidata developed the Network Common Data Form (netCDF) file format as a means of providing an open, portable standard for the storage of geoscience data for education and research. There are some key features of the netCDF file format that make it a good choice for climate scientists.^[See Unidata's [netCDF factsheet](https://www.unidata.ucar.edu/publications/factsheets/current/factsheet_netcdf.pdf).] 

- The format is self-describing: all of the metadata about the file and its contents are contained within the netCDF file, without the need for an external metadata server or file. 
- NetCDF files are portable across different types of computers. 
- The netCDF file also allows for versatility in analysis due to its efficient storage nature, the specifics of which are beyond the scope of this preface.   
- Perhaps most importantly, support for netCDF files is active and ongoing, and software has been written to access these files in Python, R, and many other programming languages on Linux, Mac, Windows, and more.

The above benefits of netCDF files have led to the format being endorsed by a number of significant government and academic agencies dedicated to the management of geoscience data. For more details on the netCDF file format, you can visit the [netCDF documentation](https://www.unidata.ucar.edu/software/netcdf/docs/index.html).

The `ncdump` program is a command-line utility (run in `bash`) that is a useful tool to begin to understand the contents of your netCDF files. The command will output information on the dimensions of your data set, the size of each dimension, and additional metadata like units and variable names. The following, for instance, is the output of that command when run on some miniature netCDF files that we have created for this book. You will surely see much more output if you run the tool on the files that you downloaded in Lab 2!

```{bash, echo=TRUE}
ncdump -h data/tas_Amon_CanESM2_historical_r1i1p1_198001-200512_mini.nc
```

You can check to see if `ncdump` is available on your computer by running `which ncdump`. If it is not installed, fear not! We have written an analogous utility for Python, which you can find [here](https://gitlab.com/claut/man_ccia/snippets/1734337/raw). Download it to your computer import the file, and run it like this:

```{r, include=FALSE}
source_python('ncdump.py')
```
```{python, echo=TRUE}
ncdump("data/tas_Amon_CanESM2_historical_r1i1p1_198001-200512_mini.nc")
```

In Lab 3 (Sec. \@ref(lab3)), you will learn how to read data from a netCDF file in Python. In that lab, we will focus on a single grid box, but keep in mind that the netCDF files that you acquired in Lab 2 contain much more data, and you can re-use the same files for future analyses of other areas, entire regions or even the entire globe! 

```{bash cleanup, include=FALSE}
rm ncdump.py
```