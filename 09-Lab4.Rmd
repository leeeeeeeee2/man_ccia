# Lab exercise 4: Choosing our models {#lab4}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 09-Lab4.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
source("R/init_python.R")
source("R/deco_hook.R")
```

```{python py_setup, include=FALSE}
import matplotlib as mpl
import seaborn as sns
mpl.use("agg") # Needed for the book only
mpl.rc('font', size = 12)
from matplotlib import pyplot as plt
plt.switch_backend('agg') # Might be overkill
import pandas as pd
pd.set_option('display.max_columns', 10)
```

## Introduction and setup

Now that we have reviewed some methods for selecting models from our 42 GCM options, let's see them in action. We'll start by importing the relevant libraries, as usual.

```{python, eval=FALSE, deco=list()}
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
```

Next, we need to find our downloaded anomaly data, as produced by the "Download Table" button in _Conjuntool_. In my case, I have this saved in a _data_ subdirectory. I am also using the default name used by _Conjuntool_, which is heavy on the details (for lack of better metadata capabilities in a _.csv_ file). If you have renamed your file, or saved it somewhere strange, adjust this line as necessary.

```{python, deco=list()}
file = "data/4367_-7940_bsln-1981‒2010_proj-2011‒2100_Annual.csv"
```

Now we read it using **Pandas**.

```{python, deco=list()}
dat = pd.read_csv(file)
print(dat.head())
```

Let's see what we're working with.

```{python, deco=list()}
print(dat.shape)
```

It looks like we have `r nrow(py$dat)` rows and `r ncol(py$dat)` columns of data. Let's drop our `'Var'`, `'Ensemble'` and `'Note'` columns, which we won't use in this analysis. If you want to analyze more than one ensemble member, you may need to hold on to the `'Ensemble'` column at least.

```{python, deco=list()}
dat = dat.drop(['Var', 'Ensemble', 'Note'], axis = 1)
print(dat.head())
```

## Selecting our models

### Using the extremes

Choosing our models based on the highest and lowest projected changes allows us to get an idea of the "range" of possibilities for the future. Since we are using change factors, we are only considering the changes that each model projects over its own baseline period. So these changes are relative. i.e. The model with the largest projected change is not necessarily the model with the highest projected temperature. 

```{block, type='rmddisclaimer'}
For this and all of our selection methods, I'll made a copy of `dat` to work with using the `copy()` method. This is the safer option, as something like `dat_ext = dat` creates a new reference to the _same_ `dat` table. This isn't an issue if we are just sub-setting and filtering data, but it becomes troublesome if we want to change any of the data points; any modification to the values in `dat_ext` would also appear in `dat`! This will become more relevant later in this lab when we create the `tor_win` data frame. 
```

```{python, deco=list()}
dat_ext = dat.copy()
```

I am going to add a new column, called `'MeanAnom'`, which will contain the average tridecadal anomaly, or change factor, produced by each model. This will be sorted from smallest change to largest.


```{python, deco=list()}
dat_ext['MeanAnom'] = dat_ext.iloc[:,-3:].mean(axis = 1)
dat_ext = dat_ext.sort_values('MeanAnom')
print(dat_ext.head()) # Smallest
print(dat_ext.tail()) # Largest
```

The next code block is a little involved. I am going to group `dat_ext` by `'Scenario'` and get the smallest value for each using `first()`. Make sure you sorted the values in the previous code block or you won't get the expected output! I will label those values "Lower", to represent the lower limit of the model projections, and restore the index, which was changed by `groupby`. I then do the same for the `last()` value, labelling these as "Upper". I append these two tables together, and drop the `'MeanAnom'` column.


```{python, deco=list()}
lower = dat_ext.groupby('Scenario').first().assign(Limit = "Lower").reset_index()
upper = dat_ext.groupby("Scenario").last().assign(Limit = "Upper").reset_index()
dat_ext = lower.append(upper).drop("MeanAnom", axis = 1)
print(dat_ext)
```

Now we have a table of the two extreme models for each scenario that identifies each model as providing either an upper or lower extreme. Great! This table suits our needs for this analysis.

### Validating model baselines

Another model selection method is to validate a model against observed baseline values. Before we can validate, we need data to validate against. Here I am using the very same _.csv_ file that we generated in Lab 1. If you no longer have that file, give the code in Lab 1 a quick run and you should be able to re-create it in no time. Note that when we saved the file using `tor.to_csv()`, an index column was included in the file. Use the `index_col` argument to let **Pandas** know about this, or you will end up with an extra unnamed column of numbers from 0 to the number of rows in the file.


```{python, deco=list()}
tor = pd.read_csv("tor.csv", index_col = 0)
print(tor.head())
```

We can use the Gough&ndash;Fenech Confidence Index (GFCI) to help us choose our models. The GFCI quantifies the difference in the model-projected baseline as a proportion of the standard deviation of the aggregate observed values using the following equation: 

$$\textrm{GFCI} = \frac{ \left | T_{\textrm{mod}} - T_{\textrm{obs}} \right |}{\sigma_{\textrm{obs}}}$$


$\left | T_{\textrm{mod}} - T_{\textrm{obs}} \right |$ is the absolute difference between the modelled baseline and the observed baseline, and $\sigma_{\textrm{obs}}$ is the standard deviation of the aggregate observed values. In my case, I need to use the mean and standard deviation of the annual aggregate values of our observed Toronto station data. If I want to test for $T_{\textrm{mean}}$, for instance, I would first calculate the annual average $T_{\textrm{mean}}$ for each of the 30-years in our baseline, and then take the mean and standard deviation of those 30 annual values.

```{block, type="rmdtip"}
If you are performing a monthly or seasonal analysis, you should use the respective aggregate values. As an example, if you are performing your analysis on winter values, $T_{\textrm{obs}}$ is the average yearly winter temperature and $\sigma_{\textrm{obs}}$ is the standard deviation of those average yearly winter temperatures. You should, of course, also have chosen seasonal output from _Conjuntool_. 
```

**Pandas** doesn't reliably process date information in a _.csv_ file, so we need to fix the `'Date'` column.

```{python, deco=list()}
tor.Date = pd.to_datetime(tor.Date)
```

Now we can generate our annual average $T_{\textrm{mean}}$.

```{python, deco=list()}
tor_mean = tor[['Date', 'MeanTemp']].groupby(tor.Date.dt.year).mean()
print(tor_mean.head())
```

Now we calculate the mean and the standard deviation of those values. 

```{python, deco=list()}
tor_tas_mean = tor_mean.MeanTemp.values.mean()
tor_tas_std = tor_mean.MeanTemp.values.std()

print("Mean:", tor_tas_mean, "... Standard Deviation:", tor_tas_std)
```

Since we averaged our baselines using _Conjuntool_, we don't need multiple baselines per model. Let's get the unique Model and Baseline combinations.

```{python, deco=list()}
dat_gf = dat.copy()[['Model', '1981-2010']].drop_duplicates()
```

Now we calculate the GFCI.

```{python, deco=list()}
dat_gf = dat_gf.assign(GFCI = abs(dat_gf[['1981-2010']] - tor_tas_mean) / tor_tas_std)
print(dat_gf.sort_values('GFCI').head())
```

The GFCI defines confidence as follows:

  - Values below 1.0 indicate that the bias in the model projections was within one standard deviation of the observed aggregate values. These values are further subdivided:
  - Values between 0.5 and 1.0 are considered to merit moderate confidence
  - Values below 0.5 are considered to merit high confidence
  - Values above 1.0 indicate that the model bias exceeds one standard deviation of the aggregate values. These projections are considered to be suspect and should not be used.

We can whip this into a simple function so that we can easily classify the confidence that we have in our models. 

```{python, deco=list()}
def confidence(GFCI):
    if (GFCI < 0.5):
        return "high"
    elif (GFCI > 1.0):
        return "low"
    else:
        return "moderate"
```

```{python, deco=list()}
dat_gf['Confidence'] = dat_gf.GFCI.apply(confidence)
print(dat_gf.sort_values('GFCI').head(10))
```

Let's see this information in a figure!

```{python, deco=list()}
plt.figure(figsize=(10,10/1.66))
fg = sns.scatterplot(x="Model", y="GFCI", hue="Confidence", data=dat_gf)
plt.title(r'GFCI Values for Model-Projected $T_\mathrm{mean}$ at Toronto (1981‒2010)')
for label in fg.axes.get_xticklabels():
    label.set_rotation(90)
plt.tight_layout()
```

```{python, include=FALSE}
plt.savefig('l4f1.png')
plt.clf()
```

```{r l4f1, echo=FALSE, fig.cap="GFCI confidence classes for GCM-projected $T_\\mathrm{mean}$ at Toronto (1981\u20122010). Lower values indicate less bias in the model projections compared to baseline annual observed average $T_\\mathrm{mean}$"}
knitr::include_graphics("l4f1.png", dpi = NA)
```

Let's get just the top three models

```{python, deco=list()}
dat_gf = dat_gf.sort_values('GFCI').iloc[0:3,:]
print(dat_gf)
```

Now that we have our models, we need to recover the change factors for these models! We can do this with a left merge of `dat` into `dat_gf`.


```{python, deco=list()}
dat_gf = pd.merge(dat_gf, dat, how = 'left', on = ['Model', '1981-2010'])
print(dat_gf)
```

```{block, type="rmddisclaimer"}
You may have noted that some of the models in our ranking have similar names. This is a good reminder of our caveat about model independence (see Sec. \@ref(selectioncaveats)). Among our top three models, MIROC-ESM and MIROC-ESM-CHEM are almost identical models. Even at the lower end of our confidence range, CESM and CCSM4 are also very closely related. We have ranked our models under the assumption that the models are independent, but in reality our models are not independent. In our example, therefore, one of our top three models will contribute little by the way of new knowledge for our analysis. In this sense, ranking models by their output value may in fact provide a better indicator of model interdependence than of model skill. For more on model independence, see this [blog post](http://www.realclimate.org/index.php/archives/2018/07/model-independence-day/) from [RealClimate](http://www.realclimate.org/).
```

### Using the multi-model ensemble

Using the ensemble is as simple as taking the average for each scenario across each scenario.

```{python, deco=list()}
dat_ens = dat.copy().groupby(['Scenario']).mean().reset_index()
print(dat_ens)
```

You may be concerned that the baseline values are different for the difference scenarios. If you are thinking, "didn't we average the baselines between all the scenarios earlier". You're right. What we're seeing here is an inconsistency based on the number of models that provide projections for each scenario. Since we aren't doing any validation here, it doesn't really matter, but it is nice to know how many models are providing projections for each scenario and period.

```{python, deco=list()}
print(dat.groupby(['Scenario']).count().reset_index())
```

From the above table, it becomes pretty obvious why most studies use RCP4.5 and RCP8.5, they are by far the most modelled. We'll use those scenarios when we apply our change factors, next.

```{block, type="rmdcomment"}
The methods that we have seen above are quick and useful for pedagogy, but there are lots of other, more advanced ways to choose models for CCIA studies. A lot of recent study has been put toward the identification of optimal sub-ensembles, using a handful of the most relevant models. You should do some independent reading on some of these methods to ensure that your toolbox is up-to-date!
```

## Applying our change factors

Now that we have chosen our models, let's see how we can apply these change factors to our observed data. For each period, we apply the given change factor to the daily values from our observed data. If we were to graph this, it would look a little bit like a jagged staircase, with each tridecade shifting by some fixed amount. It is not, however, the daily values that are of interest. We cannot reliably forecast daily weather far into the future. Instead, we use the past history, combined with the average tridecadal change to estimate changes in our exposure unit. Since I am looking at $T_{\textrm{mean}}$ here, I won't examine tropical nights as I did in lab 1. Here I will use summer cooling degree days and winter heating degree days for the RCP4.5 and RCP8.5 scenarios.

```{block, type="rmddisclaimer"}
In these examples, I am using annual change factors to analyze seasonal exposure units. This is lazy at best and downright wrong at worst. In your research, make sure to use the appropriate change factors for your period of interest. 
```

I will begin this section by showing how we can manually apply the change factors, and then I will define a function that will make this easier.

### Using the extremes

Let's begin by filtering our extreme table to only use the RCP 4.5 and RCP 8.5 scenarios.

```{python, deco=list()}
dat_ext = dat_ext[dat_ext.Scenario.isin(['RCP4.5', 'RCP8.5'])]
print(dat_ext)
```

Let's apply the high extreme for RCP 4.5 for the 2020s. There are a number of ways of sub-setting the table, above, use whichever is easiest for you.

```{python, deco=list()}
anom = dat_ext.loc[dat_ext.Model == 'MIROC-ESM-CHEM', '2011-2040'].values
print(anom)
```

Now I'll apply that anomaly to the daily `'MeanTemp'` values in a copy of the `tor` data frame.

```{python, deco=list()}
tor_ext = tor.copy()[['Date', 'MeanTemp']]
tor_ext = tor_ext.assign(hex2020s = tor.MeanTemp + anom)
print(tor_ext.head())
```

Now we can (re)calculate our exposure unit using a lambda expression just as we did in Lab 1.

```{python, deco=list()}
tor_ext['CDDObs'] = tor_ext.MeanTemp.apply(lambda x: 0 if x <= 18 else x - 18)
tor_ext['CDDhex2020s'] = tor_ext.hex2020s.apply(lambda x: 0 if x <= 18 else x - 18)
print(tor_ext.head())
```

So, what is the projected change to summertime CDD? Let's take a look.

```{python, deco=list()}
print(tor_ext[tor_ext.Date.dt.month.isin([6,7,8])].filter(regex = '^CDD').sum())
```

It looks like we can expect an increase from `r suppressWarnings(sum(py$tor_ext$CDDObs[format(py$tor_ext$Date, format = "%m") %in% sprintf("%02d", c(6,7,8))], na.rm = TRUE))` total CDDs from the baseline to $`r suppressWarnings(sum(py$tor_ext$CDDhex2020s[format(py$tor_ext$Date, format = "%m") %in% sprintf("%02d", c(6,7,8))], na.rm = TRUE))`$ in the 2020s. This could have serious implications for energy supply for air conditioning! 

Great! That's one projection down; 11 more to go to fill in our first table. This looks like it could turn out to be a monotonous process. Let's whip up a function to do the work for us! This is loosely based on [code](https://gitlab.com/ConorIA/CdeCPeru-CC/blob/master/Ejercicios/recalc_exp.R) that I put together in R in 2015. This version takes five arguments: `obs` is a two-column Data Frame containing the `'Date'` and variable column; `anoms` is the respective `dat_*` table; `stat` is the stat that we want to calculate for the exposure units, e.g. `sum`; `expr` is a lambda expression that we evaluate to calculate the exposure unit (some examples are in the Appendix of this book); finally, `month` is a list of one or more months to filter on, defaulting to `None`. If in doubt, the function has a complete docstring to help you to choose values for each parameter.

```{python, deco=list()}
from math import isnan

def recalc_exp(obs, anoms, stat, expr, month = None):
    """Recalculate and aggregate an exposure unit using observed data
    and Conjuntool output.

    Parameters
    ----------
    obs   : pandas.core.series.Series
        A single-column or array data frame with 'Date' and some variable column
    anoms : pandas.core.frame.DataFrame
        The data frame produced by Conjuntool, filtered as necessary
    stat  : builtin_function_or_method or function
        The function to call on the calculated exposure unit, e.g. sum
    expr  : str
        A lambda expression to be applied to the variable column to
        calculate the exposure unit
    month : list
        A list of the months to filter on (optional)
    """
    row_list = []
    if month is not None:
        obs = obs[obs.Date.dt.month.isin(month)]
    baseline = stat(obs.iloc[:,1].apply(eval(expr)))
    for model in set(anoms.Model):
        for scen in set(anoms.Scenario):
            periods = {}
            for period in [col for col in anoms.columns if '-' in col][1:]:
                anom = anoms.loc[(anoms.Scenario == scen) & (anoms.Model == model)][[period]].values
                if len(anom) == 0:
                    continue
                else:
                    anom = anom[0]
                tmp = obs.assign(new = obs.iloc[:,1].values + anom)
                tmp = stat(tmp.new.apply(eval(expr)))
                periods[period] = tmp
            if all(isnan(value) for value in periods.values()):
                continue
            else:
                row = {'Model': model,
                       'Scenario': scen,
                       'Baseline': baseline}
                row = {**row, **periods}
                row_list.append(row)
    return pd.DataFrame(row_list, columns=row.keys())
```

Let's take a look at CDDs, based on our extreme models.

```{python, deco=list()}
print(recalc_exp(obs=tor[['Date', 'MeanTemp']],
                 anoms=dat_ext,
                 stat=sum,
                 expr="lambda x: 0 if x <= 18 else x - 18",
                 month=[6,7,8]))
```

As we might have expected, there is a larger increase in CDDs for the RCP 8.5 Scenario than for the RCP 4.5 scenario. 

The above table is missing an important piece of information. In our `dat_ext` table, we had labelled each model as providing and upper or lower extreme, but this information is not preserved by the `recalc_exp()` function. How can we recover the information? The simplest option might be to replace the values in the `dat_ext.Model` with the values of `dat_ext.Limit`, but this, again, removes some key information from the results, specifically, the model that provided each projection. Another option would be to re-write `recalc_exp()` so that it preserves both pieces of information. The third option, however, is the one that we will use: we already _know_ whether each model in our `recalc_exp()` output represents the upper or lower limit of our projections, since that information is present in `dat_ext`. We can use the same mechanism of merging two tables that we used in in the GFCI section to create a table that includes both pieces of data. 

```{python, deco=list()}
projected_cdds = recalc_exp(obs=tor[['Date', 'MeanTemp']],
                            anoms=dat_ext,
                            stat=sum,
                            expr="lambda x: 0 if x <= 18 else x - 18",
                            month=[6,7,8])

print(pd.merge(projected_cdds, dat_ext[['Scenario', 'Model', 'Limit']]))
```

We can also use our `recalc_exp()` function to give us the 30-year daily average CDD, instead of the 30-year total. 

```{python, deco=list()}
from numpy import mean

print(pd.merge(recalc_exp(obs=tor[['Date', 'MeanTemp']],
                          anoms=dat_ext,
                          stat=mean,
                          expr="lambda x: 0 if x <= 18 else x - 18",
                          month=[6,7,8]),
               dat_ext[['Scenario', 'Model', 'Limit']]))
```

```{block, type="rmdassignment"}

#### Bonus exercise

The `recalc_exp()` function works on the daily values of each 30-year tridecade. Thus, it returns either the daily average, or the tridecadal total of a given exposure unit. These values might not be the most useful metrics for most analysts. It might be more useful to know what we might expect the _annual_ average or sum for a given metric. In our example the seasonal or annual total CDDs might be more useful to policy-makers.

For bonus marks: modify the `recalc_exp()` function above so that it first aggregates the daily values by year (`obs.Date.dt.year`), and then applies the calculation provided by the `stat` argument. For full marks, your function should also retain the original functionality of `recalc_exp()`. (_Hint: you may need to add an extra parameter to the function_).

Once you have finished changing the code, generate a patch for the original function using software such as [Pretty Diff](https://prettydiff.com/). This will highlight the changes that you made. Submit this patch along with your regular "What to submit" exercises. 
```

What if we want to calculate winter heating degree days? We'll need to shift our Decembers up a year like we did in Lab 1. This isn't hugely necessary at the moment, since I'm looking at 30-year totals, but it is always a good habit to maintain and allows us to be more precise about the seasons in consideration. It will become especially relevant for those of your who undertake the bonus assignment in the box above.

```{python, deco=list()}
tor_win = tor.copy()
tor_win['Date'] = tor_win.Date.apply(lambda x: x + pd.DateOffset(years=1) if x.month == 12 else x)
print(tor_win.head()) # Not shifted
print(tor_win.tail()) # Shifted
```

Let's drop the years that we know to be incomplete, winter 1981/82, which is missing December 1980, and winter 2010/11, which is just December 2010. Remember, a better solution for the former would be to download data for December 1980 so that we can include all 30 winters. 

```{python, deco=list()}
tor_win = tor_win[tor_win.Date.dt.year.isin(range(1982, 2011))]
```

```{python, deco=list()}
print(pd.merge(recalc_exp(obs=tor_win[['Date', 'MeanTemp']],
                          anoms=dat_ext,
                          stat=sum,
                          expr="lambda x: 0 if x >= 18 else 18 - x",
                          month=[12,1,2]),
               dat_ext[['Scenario', 'Model', 'Limit']]))
```

As winter is projected to warm, we see less heating degree days. This change is more rapid in the scenario with more radiative forcing. 

### Using the multi-model ensemble

Since the `recalc_exp()` function expects our `anoms` data frame to include a `'Model'` column, we should add one.


```{python, deco=list()}
dat_ens['Model'] = "Ensemble"
print(dat_ens)
```

Now we can take a look at the projected changes in our exposure units.

For CDD:

```{python, deco=list()}
print(recalc_exp(obs=tor[['Date', 'MeanTemp']],
                 anoms=dat_ens[dat_ens.Scenario.isin(['RCP4.5', 'RCP8.5'])],
                 stat=sum,
                 expr="lambda x: 0 if x <= 18 else x - 18",
                 month=[6,7,8]))
```

For HDD:

```{python, deco=list()}
print(recalc_exp(obs=tor_win[['Date', 'MeanTemp']],
                 anoms=dat_ens[dat_ens.Scenario.isin(['RCP4.5', 'RCP8.5'])],
                 stat=sum,
                 expr="lambda x: 0 if x >= 18 else 18 - x",
                 month=[12,1,2]))
```

### Using our top-ranked models

How about our top ranked models? Our function works as expected for CDD:

```{python, deco=list()}
print(recalc_exp(obs=tor[['Date', 'MeanTemp']],
                 anoms=dat_gf[dat_gf.Scenario.isin(['RCP8.5', 'RCP4.5'])],
                 stat=sum,
                 expr="lambda x: 0 if x <= 18 else x - 18",
                 month=[6,7,8]))
```

OK, enough tables, let's plot some of these changes! First, I will create a table of results. Called `out`.

```{python, deco=list()}
out = recalc_exp(obs=tor_win[['Date', 'MeanTemp']],
                 anoms=dat_gf[dat_gf.Scenario.isin(['RCP4.5', 'RCP8.5'])],
                 stat=sum,
                 expr="lambda x: 0 if x >= 18 else 18 - x",
                 month=[12,1,2])
print(out)
```

In Lab 3, we used `pd.pivot()` to turn our table from a narrow layout to a wide layout. I stressed, however, that the narrow layout was more efficient for plotting. This time around, we have a wide data frame, which we want to convert to a narrow one. The method that we need to call, the antithesis to `pd.pivot()` is `pd.melt()`. 

```{python, deco=list()}
out_long = out.melt(id_vars = ['Model', 'Scenario'], var_name = "Period", value_name = "Cooling Degree Days")
```

Now we are ready to plot! Play around with the options in the **seaborn** (`sns`) plot below until you've come up with a figure you are happy with. Make sure to add complete labels (like a _title_, for instance).

```{python, deco=list()}
sns.catplot(x = "Period", y = "Cooling Degree Days", hue = "Scenario", col = "Model",
            data = out_long, hue_order=["RCP4.5", "RCP8.5"], saturation = 0.5,
            palette = "Reds", kind = "bar", ci = None, height = 7, aspect = 0.66)
```

```{python, include=FALSE}
plt.savefig('l4f2.png')
plt.clf()
```

```{r l4f2, echo=FALSE, fig.cap="Baseline and future summer cooling degree days (CDD) at Toronto. Projections are based on changes to mean temperature forecast by three GCMs under the RCP4.5 and RCP8.5 scenarios."}
knitr::include_graphics("l4f2.png", dpi = NA)
```

Great! We have now seen who we can apply GCM-projected change factors to our observed data to project changes in an exposure unit. In our next lab, we will look at more advanced downscaling methods.

## Exercises (what to submit)

```{block, type='rmdassignment'}
- Choose either summer tropical nights or winter FDD. Go back to Conjuntool and select the relevant seasonal anomaly data. Describe differences in the results between the annual change factors and the seasonal ones. [2 marks]
- Are the models that best reproduced the _annual_ baseline the same models that best reproduce the _seasonal_ baseline? If not, can we trust our validation results? [2 marks]
- Re-calculate the changes to the exposure unit using the _seasonal_ data. What differences do you see compared to the annual data? [2 marks]
- Is it better to use the annual data or the seasonal data for a seasonal study? Why? [2 marks]
- Produce two multi-panel plots similar to the Figure 10.2. One figure should plot the changes projected by the top-three "validated" models. The other should plot the changes projected by the two extremes and the ensemble average. Make sure that the figures are properly labelled and properly captioned. _Hint: you may find it useful to assign a dummy `"Limit"` variable to the ensemble projections just like we assigned a dummy `"Model"` variable to `dat_ens`. You should also adjust the `id_vars` in `pd.melt()`, and adjust the `col` facet in `sns.catplot()`. If you are totally stuck, you can see an [example here](https://gitlab.com/claut/man_ccia/snippets/1830781)._ [2 marks]
- Compare, contrast, and comment on the three model selection methods explored in this lab. What are their strengths and weaknesses? [5 marks]
```
