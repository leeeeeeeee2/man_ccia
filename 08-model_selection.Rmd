# Strategies for selecting GCM models {#modelselection}

_Last update: `r system("git log -1 --format=\"%ad (%h)\" -- 08-model_selection.Rmd", intern = TRUE)`_

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readr)
library(dplyr)
library(plotly)
library(stringr)
```

One of the challenges facing students and practitioners of climate change impact assessment is deciding which models to use for the assessment. Have a look at Figure \@ref(fig:ts-plot). The black line is the actual observed Toronto annual average $T_{\textrm{mean}}$ (produced from the data that we downloaded from Environment and Climate Change Canada). Each of the coloured solid lines represents the historical scenario of some model from the r1i1p1 ensemble. The dashed lines, starting circa 2005, represent the RCP scenario-forced projections for future climate in the same grid cell. 

```{r ts-plot, message=FALSE, warning=FALSE, fig.cap="Observed and GCM-modelled annual average mean temperature at Toronto (1981\u20122100). The black line traces observed data from the Toronto climate station. Each coloured line represents the projection from one of 39 CMIP5 GCM models. Solid lines indicate historical projections; broken lines are projections forced with the four IPCC RCP scenarios. Mouse over the figure for a detailed tooltip."}

tor <- read_csv("tor.csv") %>% select(-X1)
source("R/aggregate_with_missing.R")
tor_annual <- aggregate_with_missing(tor)
rm(tor, aggregate_with_missing)

tor_anoms <- read_csv("data/cjt_tor_anoms.csv")
models <- tor_anoms %>%
  filter(!(Model %in% c("HadCM3", "MIROC4h"))) %>%
  mutate(Model = gsub("([0-9])-([0-9])", paste0("\\1", ".", "\\2"),
                      gsub("([0-9])-([0-9])", paste0("\\1", ".", "\\2"), Model))) %>%
  select(Model) %>% unlist %>% unique

gcm  <- readRDS("data/cjt_tor_tas_fullts.rds") %>%
  filter(Model %in% models & Year >= 1981 & Year <= 2100) %>%
  group_by(Var, Model, Scenario, Ensemble, Year) %>%
  dplyr::select(-Time, -Month) %>%
  summarize(Value = mean(Value))

gcm <- gcm %>% ungroup %>% mutate(Scenario = case_when(
  str_detect(Scenario, "historical/") & Year <= 2005 ~ "historical",
  str_detect(Scenario, "historical/") & Year >= 2006 ~ sub("historical/", "", Scenario),
  TRUE ~ Scenario)) %>%
  filter(!(str_detect(Model, "HadGEM") & Year == 2005 & Scenario != "historical")) %>%
  distinct()

to_plot <- tor_annual %>% mutate(Scenario = "historical", Value = MeanTemp)

p_gcm <- plot_ly(data = gcm, x = ~Year, y = ~Value, color = ~Model, linetype = ~Scenario,
                 hoverinfo = "text",
                 hovertext = sprintf("%s: %s (%s): %2.1f°C", gcm$Model, gcm$Year,
                                     gcm$Scenario, gcm$Value)) %>%
  add_lines() %>% add_lines(data = to_plot, color = I("black"), hoverinfo = "text",
                 hovertext = sprintf("Observed: %s, (historical): %2.1f°C", to_plot$Year,
                                     to_plot$Value)) %>%
  layout(margin = list(b = 60),
         yaxis = list(title = "<i>T</i><sub>mean</sub> (°C)"), 
         showlegend = FALSE)

p_gcm
```

So, why do we see so much variation? There are a couple of factors at play here. Perhaps most importantly, is the fact that we are comparing apples to oranges, or, more specifically, an apple to an orchard! Each of these model projections is for a _grid cell_ that contains the specific coordinates of the Toronto weather station. Toronto has a lot of local climate considerations like a strong Urban Heat Island and an important Lake Breeze, which makes it stand out in the context of the larger region. These processes are not well represented in the climate models, some of which don't even consider the Great Lakes!

Another consideration is the fact that each model will display its own bias. The modelling groups that have created these models only focus on "local areas" in the broadest sense of the word (e.g., the Arctic), unless there is a very specific, large bias in a particular region that needs addressing. Biases typically arise due to biases in the representation of physical processes (usually in parametrization) or due to biases generated by inadequate horizontal or vertical resolution. Just like the climate of a model, the biases are emergent and are actually quite hard to remedy on a local scale. These biases mean that a model that performs very well over the deserts of Australia may not perform as well over the Arctic. 

How, then, do we choose which models to use to inform our climate change impact assessment? There are a number of strategies that we can employ for the task, including using models at the extreme top and bottom of the projected range, using the average of all models in a multi-model ensemble, or selecting "validated" models based on the models' ability to reproduce the observed baseline climate. We will detail these strategies in the following section.

## Model selection strategies 

@fenech2007selecting describe three approaches to selecting GCM models: using the extremes; validating models based on their ability to reproduce some observed metric; and the use of multi-model ensembles.

If we select the model extremes, we would select both the smallest and the largest projected changes as reference limits for policy planning. In this sense, the extreme models can provide an indicator of the upper and lower limits of the the full range of possible futures projected by the available GCMs.

Another common approach to model selection is validated models based on their ability to simulate the past and present climate. Some studies that have used validation methods include @ragettli2013sources, @xie2013characteristics, and @hewer2016assessing. When we use a validation-based approach, we are working under the assumption that a model's ability to simulate an observed baseline climate will be representative of the same model's ability to project future climate. 

Scientists may also choose to use all (or a subset) of the available models to create a multi-model ensemble. In a multi-model ensemble, the output from the selected models is averaged, and that average is used to project future climate. Multi-model ensembles often outperform individual models [@pierce2009selecting]. The ensemble approach partially addresses the varying strengths and weaknesses of individual models, with the hope that model biases will be cancelled out within the averaged output [@knutti2010challenges]. The ensemble approach has been shown to cancel offsetting errors present in individual models [@pierce2009selecting], and can correct for differences in the parametrization of the models [@knutti2010challenges]. Thus, some studies may choose to make use of a full ensemble of all available models, to account for the entire range of available projection data, such as @gough2016ontario.

## Caveats to these strategies {#selectioncaveats}

The above strategies are easy to implement, and are commonly used in the literature, however, there are some important caveats and criticisms to these methods. For instance, a key criticism levelled against the use of the upper and lower extremes is that we typically do not have _a priori_ information regarding _why_ the relevant models offer extreme projections. Perhaps they truly are outliers, and we are creating an artificially wide range of possible scenarios.

Model validation is also problematic. Recall our earlier metaphor about an apple in an orchard. The spatial scales between the station data and the GCM grid are very different, and judging global model performance based on a sub grid-point observed data is like burning our orchard to the ground because we found a worm in a single apple. However, this mismatch does not only apply to the spatial scale. We also need to consider the temporal scale: Assuming a model simulates variability correctly, it can be hard to know whether this variability agrees with observations because we have a very limited historical record, and therefore a limited sample of observations. Modelled climatological processes can vary dramatically over the course of a long-term simulation. Sometimes this variability will line up well with our observations, and sometimes it won't. If, upon validation, we happen to choose the model years in which our simulation does not look like observations, then our model will seem flawed. However, it could be the case that nature is actually very similar to our model, but we've only sampled one small piece of it’s total variability. Finally, and perhaps most importantly, we can't be sure that the ability of a model to project the past is truly representative of the same model's ability to project the future. In fact, even the ability of the model to reproduce the baseline presents a challenge related to equifinality; @gough2001model showed that changes in model parametrization can greatly change the resulting future projections, even if the baseline is reproduced equally well by each run.

Finally, all three of our methods generally consider the available models to be independent options for the future. In truth, many of the CMIP5 models have a shared history, or use the same underlying code for some of their components (atmosphere, ocean, ice, land surface). In this sense, the models are not truly independent. If we perform a validation approach to select the top-ranked models, it is likely that we will find that interdependent models perform equally well or equally poorly. Likewise, in an ensemble-based study, including related models goes against our goal of cancelling out individual biases, as two interdependent models could artificially skew the overall mean slightly. 

Given the above, we must carefully consider our approach to model selection to ensure that we are making good choices. In Lab 4 (Sec. \@ref(lab4)) we will compare these model selection methods using a change factor approach. In Lab 5 (Sec. \@ref(lab5)), we will see how model projections can be downscaled to a smaller scale to reduce the model bias and, therefore, increase model skill, without the need for validation-based model selection approaches.
